{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10420483,"sourceType":"datasetVersion","datasetId":6458527},{"sourceId":10421059,"sourceType":"datasetVersion","datasetId":6458944},{"sourceId":10442681,"sourceType":"datasetVersion","datasetId":6463563},{"sourceId":10450853,"sourceType":"datasetVersion","datasetId":6469112},{"sourceId":10451956,"sourceType":"datasetVersion","datasetId":6469923},{"sourceId":10453067,"sourceType":"datasetVersion","datasetId":6470727}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch-lightning torch torchvision transformers numpy\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:10:47.257055Z","iopub.execute_input":"2025-01-12T18:10:47.257342Z","iopub.status.idle":"2025-01-12T18:10:51.749825Z","shell.execute_reply.started":"2025-01-12T18:10:47.257308Z","shell.execute_reply":"2025-01-12T18:10:51.748738Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.4.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.5)\nRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\nRequirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\nRequirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.6.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.11.9)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (71.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:10:54.162215Z","iopub.execute_input":"2025-01-12T18:10:54.162495Z","iopub.status.idle":"2025-01-12T18:10:55.789971Z","shell.execute_reply.started":"2025-01-12T18:10:54.162474Z","shell.execute_reply":"2025-01-12T18:10:55.788922Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'yolov5'...\nremote: Enumerating objects: 17129, done.\u001b[K\nremote: Counting objects: 100% (73/73), done.\u001b[K\nremote: Compressing objects: 100% (57/57), done.\u001b[K\nremote: Total 17129 (delta 39), reused 21 (delta 16), pack-reused 17056 (from 3)\u001b[K\nReceiving objects: 100% (17129/17129), 15.81 MiB | 35.04 MiB/s, done.\nResolving deltas: 100% (11741/11741), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%cd yolov5\n!pip install -r /kaggle/working/yolov5/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:10:57.184625Z","iopub.execute_input":"2025-01-12T18:10:57.184989Z","iopub.status.idle":"2025-01-12T18:11:01.535388Z","shell.execute_reply.started":"2025-01-12T18:10:57.184961Z","shell.execute_reply":"2025-01-12T18:11:01.534579Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/yolov5\nRequirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 5)) (3.1.43)\nRequirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 6)) (3.7.1)\nRequirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 7)) (1.26.4)\nRequirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 8)) (4.10.0.84)\nRequirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 9)) (10.4.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 10)) (5.9.5)\nRequirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 11)) (6.0.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 12)) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 13)) (1.13.1)\nCollecting thop>=0.1.1 (from -r /kaggle/working/yolov5/requirements.txt (line 14))\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 15)) (2.4.1+cu121)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 16)) (0.19.1+cu121)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 17)) (4.66.5)\nCollecting ultralytics>=8.2.34 (from -r /kaggle/working/yolov5/requirements.txt (line 18))\n  Downloading ultralytics-8.3.59-py3-none-any.whl.metadata (35 kB)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 27)) (2.1.4)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 28)) (0.12.2)\nRequirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 42)) (71.0.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython>=3.1.30->-r /kaggle/working/yolov5/requirements.txt (line 5)) (4.0.11)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (1.3.0)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (4.53.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (24.1)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (2.8.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r /kaggle/working/yolov5/requirements.txt (line 12)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r /kaggle/working/yolov5/requirements.txt (line 12)) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r /kaggle/working/yolov5/requirements.txt (line 12)) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r /kaggle/working/yolov5/requirements.txt (line 12)) (2024.8.30)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (2024.6.1)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.2.34->-r /kaggle/working/yolov5/requirements.txt (line 18)) (9.0.0)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics>=8.2.34->-r /kaggle/working/yolov5/requirements.txt (line 18))\n  Downloading ultralytics_thop-2.0.13-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r /kaggle/working/yolov5/requirements.txt (line 27)) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r /kaggle/working/yolov5/requirements.txt (line 27)) (2024.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r /kaggle/working/yolov5/requirements.txt (line 5)) (5.0.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (1.3.0)\nDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nDownloading ultralytics-8.3.59-py3-none-any.whl (906 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m906.8/906.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.13-py3-none-any.whl (26 kB)\nInstalling collected packages: ultralytics-thop, thop, ultralytics\nSuccessfully installed thop-0.1.1.post2209072238 ultralytics-8.3.59 ultralytics-thop-2.0.13\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport pytorch_lightning as pl\n\n# Define paths\nvideo_dir = \"/kaggle/input/cholec-train-data/CholecT50/videos\"\nlabel_dir = \"/kaggle/input/cholec-train-data/CholecT50/labels\"\n\n# Training and testing video sub-folder names\ntrain_videos = [\"VID01\", \"VID02\", \"VID04\", \"VID05\", \"VID06\", \"VID08\", \"VID10\", \"VID12\", \"VID13\", \"VID14\"]\ntest_videos = [\"VID92\", \"VID96\", \"VID103\", \"VID110\", \"VID111\"]\n\ndef parse_annotations(label_path):\n    \"\"\"Parse JSON annotations and extract bounding boxes and triplet data.\"\"\"\n    with open(label_path, 'r') as file:\n        data = json.load(file)\n    annotations = data['annotations']\n\n    parsed_data = []\n    for frame_id, triplets in annotations.items():\n        for triplet in triplets:\n            parsed_data.append({\n                'frame_id': int(frame_id),\n                'triplet_id': triplet[0],\n                'instrument_id': triplet[1],\n                'verb_id': triplet[8],\n                'target_id': triplet[9],\n                'phase_id': triplet[14],\n                'bbox': triplet[2:6]\n            })\n    return parsed_data\n\ndef create_dataset(video_list, video_dir, label_dir):\n    \"\"\"Create a DataFrame for the dataset.\"\"\"\n    data = []\n    for video_id in tqdm(video_list, desc=\"Processing Videos\"):\n        video_path = Path(video_dir) / video_id\n        label_path = Path(label_dir) / f\"{video_id}.json\"\n\n        annotations = parse_annotations(label_path)\n        for entry in annotations:\n            frame_path = video_path / f\"{entry['frame_id']}.png\"\n            data.append({\n                'video_id': video_id,\n                'frame_path': str(frame_path),\n                'triplet_id': entry['triplet_id'],\n                'instrument_id': entry['instrument_id'],\n                'verb_id': entry['verb_id'],\n                'target_id': entry['target_id'],\n                'phase_id': entry['phase_id'],\n                'bbox': entry['bbox']\n            })\n    return pd.DataFrame(data)\n\n# Prepare Train and Test DataFrames\ntrain_df = create_dataset(train_videos, video_dir, label_dir)\ntest_df = create_dataset(test_videos, video_dir, label_dir)\n\n# Save DataFrames to CSV for verification\ntrain_df.to_csv(\"train_data.csv\", index=False)\ntest_df.to_csv(\"test_data.csv\", index=False)\n\n# Print DataFrame samples\nprint(\"Training Data Sample:\")\nprint(train_df.head())\nprint(\"\\nTesting Data Sample:\")\nprint(test_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:11:07.098170Z","iopub.execute_input":"2025-01-12T18:11:07.098467Z","iopub.status.idle":"2025-01-12T18:11:14.792290Z","shell.execute_reply.started":"2025-01-12T18:11:07.098445Z","shell.execute_reply":"2025-01-12T18:11:14.791468Z"}},"outputs":[{"name":"stderr","text":"Processing Videos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 19.92it/s]\nProcessing Videos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 30.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Data Sample:\n  video_id                                         frame_path  triplet_id  \\\n0    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n1    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n2    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n3    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n4    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n\n   instrument_id  verb_id  target_id  phase_id             bbox  \n0              0        0        1.0         0  [1, -1, -1, -1]  \n1              0        0        1.0         0  [1, -1, -1, -1]  \n2              0        0        1.0         0  [1, -1, -1, -1]  \n3              0        0        1.0         0  [1, -1, -1, -1]  \n4              0        0        1.0         0  [1, -1, -1, -1]  \n\nTesting Data Sample:\n  video_id                                         frame_path  triplet_id  \\\n0    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n1    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n2    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n3    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n4    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n\n   instrument_id  verb_id  target_id  phase_id             bbox  \n0             -1       -1        1.0         0  [1, -1, -1, -1]  \n1             -1       -1        1.0         0  [1, -1, -1, -1]  \n2             -1       -1        1.0         0  [1, -1, -1, -1]  \n3             -1       -1        1.0         0  [1, -1, -1, -1]  \n4             -1       -1        1.0         0  [1, -1, -1, -1]  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_df.iloc()[0][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T17:33:31.102343Z","iopub.execute_input":"2025-01-12T17:33:31.102624Z","iopub.status.idle":"2025-01-12T17:33:31.111715Z","shell.execute_reply.started":"2025-01-12T17:33:31.102603Z","shell.execute_reply":"2025-01-12T17:33:31.110919Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-6-418e02f071fb>:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  train_df.iloc()[0][1]\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/cholec-train-data/CholecT50/videos/VID01/0.png'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Load class names from class_list.txt\nclass_list_path = \"/kaggle/input/finetuning/m2cai16-tool-locations/class_list.txt\"\nwith open(class_list_path, \"r\") as file:\n    classes = [line.strip() for line in file.readlines()]\n\n# Print loaded class names\nprint(\"Loaded class names:\")\nprint(classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T17:33:34.091012Z","iopub.execute_input":"2025-01-12T17:33:34.091288Z","iopub.status.idle":"2025-01-12T17:33:34.106681Z","shell.execute_reply.started":"2025-01-12T17:33:34.091268Z","shell.execute_reply":"2025-01-12T17:33:34.105802Z"}},"outputs":[{"name":"stdout","text":"Loaded class names:\n['1 Grasper', '2 Bipolar', '3 Hook', '4 Scissors', '5 Clipper', '6 Irrigator', '7 SpecimenBag']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\nimport os\n\nannotations_dir = \"/kaggle/input/finetuning/m2cai16-tool-locations/Annotations\"\nunique_classes = set()\n\nfor annotation_file in os.listdir(annotations_dir)[:10]:  # Check first 10 files\n    annotation_path = os.path.join(annotations_dir, annotation_file)\n    tree = ET.parse(annotation_path)\n    root = tree.getroot()\n\n    for obj in root.findall(\"object\"):\n        class_name = obj.find(\"name\").text\n        unique_classes.add(class_name)\n\nprint(\"Classes found in XML annotations:\")\nprint(unique_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T17:33:36.019967Z","iopub.execute_input":"2025-01-12T17:33:36.020236Z","iopub.status.idle":"2025-01-12T17:33:36.101898Z","shell.execute_reply.started":"2025-01-12T17:33:36.020216Z","shell.execute_reply":"2025-01-12T17:33:36.101238Z"}},"outputs":[{"name":"stdout","text":"Classes found in XML annotations:\n{'Irrigator', 'Scissors', 'Clipper', 'Grasper', 'Bipolar', 'SpecimenBag'}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport cv2\nimport xml.etree.ElementTree as ET\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\n# Define paths\nannotations_dir = \"/kaggle/input/finetuning/m2cai16-tool-locations/Annotations\"\nimages_dir = \"/kaggle/input/finetuning/m2cai16-tool-locations/JPEGImages\"\nclass_list_path = \"/kaggle/input/finetuning/m2cai16-tool-locations/class_list.txt\"\noutput_dir = \"/kaggle/working/finetuning_yolo_dataset\"\n\n# Load and clean class names (strip numeric prefixes)\nwith open(class_list_path, \"r\") as file:\n    classes = [line.strip().split(' ', 1)[1] for line in file.readlines()]\n\nprint(\"Processed class names from class_list.txt:\")\nprint(classes)\n\n# Validate class names against XML\nxml_classes = set()\nfor xml_file in os.listdir(annotations_dir):\n    tree = ET.parse(os.path.join(annotations_dir, xml_file))\n    for obj in tree.getroot().findall(\"object\"):\n        xml_classes.add(obj.find(\"name\").text)\n\nprint(\"Classes found in XML annotations:\")\nprint(xml_classes)\n\n# Check if all XML classes exist in class_list.txt\nmissing_classes = xml_classes - set(classes)\nif missing_classes:\n    print(f\"Warning: Missing classes in class_list.txt: {missing_classes}\")\nelse:\n    print(\"All XML classes are present in class_list.txt.\")\n\n# Ensure output directories exist\nos.makedirs(output_dir, exist_ok=True)\nos.makedirs(f\"{output_dir}/labels/train\", exist_ok=True)\nos.makedirs(f\"{output_dir}/labels/val\", exist_ok=True)\nos.makedirs(f\"{output_dir}/images/train\", exist_ok=True)\nos.makedirs(f\"{output_dir}/images/val\", exist_ok=True)\n\n# Function to convert Pascal VOC annotations to YOLO format\ndef voc_to_yolo(annotation_path, image_width, image_height):\n    tree = ET.parse(annotation_path)\n    root = tree.getroot()\n    yolo_annotations = []\n\n    for obj in root.findall(\"object\"):\n        class_name = obj.find(\"name\").text\n        if class_name not in classes:\n            continue  # Skip unknown classes\n\n        class_id = classes.index(class_name)\n        bbox = obj.find(\"bndbox\")\n        xmin = float(bbox.find(\"xmin\").text)\n        ymin = float(bbox.find(\"ymin\").text)\n        xmax = float(bbox.find(\"xmax\").text)\n        ymax = float(bbox.find(\"ymax\").text)\n\n        # Convert to YOLO format\n        x_center = ((xmin + xmax) / 2) / image_width\n        y_center = ((ymin + ymax) / 2) / image_height\n        width = (xmax - xmin) / image_width\n        height = (ymax - ymin) / image_height\n        yolo_annotations.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n\n    return yolo_annotations\n\n# Split data into train and val sets\nannotation_files = os.listdir(annotations_dir)\ntrain_files, val_files = train_test_split(annotation_files, test_size=0.2, random_state=42)\n\n# Process and save annotations in YOLO format\nfor split, files in [(\"train\", train_files), (\"val\", val_files)]:\n    for annotation_file in tqdm(files, desc=f\"Processing {split} data\"):\n        annotation_path = os.path.join(annotations_dir, annotation_file)\n        image_file = annotation_file.replace(\".xml\", \".jpg\")\n        image_path = os.path.join(images_dir, image_file)\n        \n        # Get image dimensions\n        if not os.path.exists(image_path):\n            print(f\"Image not found: {image_path}\")\n            continue\n        \n        image = cv2.imread(image_path)\n        height, width, _ = image.shape\n\n        # Convert annotations\n        yolo_annotations = voc_to_yolo(annotation_path, width, height)\n        \n        # Save annotations\n        with open(f\"{output_dir}/labels/{split}/{image_file.replace('.jpg', '.txt')}\", \"w\") as file:\n            file.write(\"\\n\".join(yolo_annotations))\n        \n        # Copy images to the corresponding directory\n        os.system(f\"cp {image_path} {output_dir}/images/{split}/\")\n\n# Save dataset.yaml for YOLOv5 training\ndataset_yaml = f\"\"\"\ntrain: {output_dir}/images/train\nval: {output_dir}/images/val\n\nnc: {len(classes)}\nnames: {classes}\n\"\"\"\nwith open(f\"{output_dir}/dataset.yaml\", \"w\") as file:\n    file.write(dataset_yaml)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T17:33:38.664758Z","iopub.execute_input":"2025-01-12T17:33:38.665090Z","iopub.status.idle":"2025-01-12T17:34:41.619441Z","shell.execute_reply.started":"2025-01-12T17:33:38.665064Z","shell.execute_reply":"2025-01-12T17:34:41.618695Z"}},"outputs":[{"name":"stdout","text":"Processed class names from class_list.txt:\n['Grasper', 'Bipolar', 'Hook', 'Scissors', 'Clipper', 'Irrigator', 'SpecimenBag']\nClasses found in XML annotations:\n{'Irrigator', 'Scissors', 'Clipper', 'Hook', 'Grasper', 'Bipolar', 'SpecimenBag'}\nAll XML classes are present in class_list.txt.\n","output_type":"stream"},{"name":"stderr","text":"Processing train data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2248/2248 [00:38<00:00, 59.05it/s]\nProcessing val data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 563/563 [00:09<00:00, 61.99it/s]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\n\n# Disable W&B visualization by setting the environment variable\nos.environ[\"WANDB_MODE\"] = \"disabled\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T17:36:03.931921Z","iopub.execute_input":"2025-01-12T17:36:03.932235Z","iopub.status.idle":"2025-01-12T17:36:03.936098Z","shell.execute_reply.started":"2025-01-12T17:36:03.932213Z","shell.execute_reply":"2025-01-12T17:36:03.935305Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# !rm /kaggle/working/finetuning_yolo_dataset/labels/train.cache\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T15:47:43.762488Z","iopub.execute_input":"2025-01-12T15:47:43.762977Z","iopub.status.idle":"2025-01-12T15:47:43.891003Z","shell.execute_reply.started":"2025-01-12T15:47:43.762940Z","shell.execute_reply":"2025-01-12T15:47:43.889451Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"!python train.py --img 640 --batch 16 --epochs 25 --data /kaggle/working/finetuning_yolo_dataset/dataset.yaml --weights /kaggle/input/model-weight-1/best.pt --project /kaggle/working/runs --name finetune_yolov5\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T17:36:07.549287Z","iopub.execute_input":"2025-01-12T17:36:07.549555Z","iopub.status.idle":"2025-01-12T17:53:16.833042Z","shell.execute_reply.started":"2025-01-12T17:36:07.549535Z","shell.execute_reply":"2025-01-12T17:53:16.831998Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ‚úÖ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ‚ö†Ô∏è wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n2025-01-12 17:36:18.429560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-01-12 17:36:18.860403: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-01-12 17:36:18.983954: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mtrain: \u001b[0mweights=/kaggle/input/model-weight-1/best.pt, cfg=, data=/kaggle/working/finetuning_yolo_dataset/dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=25, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=/kaggle/working/runs, name=finetune_yolov5, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\nYOLOv5 üöÄ v7.0-394-g86fd1ab2 Python-3.10.12 torch-2.4.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n\n\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /kaggle/working/runs', view at http://localhost:6006/\nDownloading https://github.com/ultralytics/assets/releases/download/v0.0.0/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 15.2MB/s]\nOverriding model.yaml nc=13 with nc=7\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 24      [17, 20, 23]  1     32364  models.yolo.Detect                      [7, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\nModel summary: 214 layers, 7038508 parameters, 7038508 gradients, 16.0 GFLOPs\n\nTransferred 343/349 items from /kaggle/input/model-weight-1/best.pt\n/kaggle/working/yolov5/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast(autocast):\n/kaggle/working/yolov5/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast(autocast):\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\nWARNING ‚ö†Ô∏è DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\nSee Multi-GPU Tutorial at https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training to get started.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/finetuning_yolo_dataset/labels/train... 2248 ima\u001b[0m\n\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/finetuning_yolo_dataset/labels/train.cache\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/finetuning_yolo_dataset/labels/val... 563 images, \u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/finetuning_yolo_dataset/labels/val.cache\n\n\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.47 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\nPlotting labels to /kaggle/working/runs/finetune_yolov5/labels.jpg... \n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/kaggle/working/yolov5/train.py:355: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=amp)\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1m/kaggle/working/runs/finetune_yolov5\u001b[0m\nStarting training for 25 epochs...\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n  0%|          | 0/141 [00:00<?, ?it/s]/kaggle/working/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(amp):\n       0/24      1.92G    0.07246    0.03475    0.05354         25        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802    0.00475      0.695     0.0304    0.00817\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       1/24      2.39G    0.05501    0.02955    0.04487         25        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.445       0.11      0.122     0.0365\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       2/24      2.39G    0.05234    0.02342    0.03727         26        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.361      0.447      0.372      0.148\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       3/24      2.39G    0.04739     0.0209     0.0294         25        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.568      0.563      0.543      0.221\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       4/24      2.39G    0.04381    0.01931    0.02142         21        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.757      0.654      0.719      0.282\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       5/24      2.39G    0.04157      0.019      0.016         24        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.803      0.785      0.849      0.396\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       6/24      2.39G    0.03948    0.01818    0.01276         28        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802       0.89       0.79      0.872      0.408\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       7/24      2.39G     0.0384    0.01819    0.01057         28        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.853      0.849      0.906      0.427\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       8/24      2.39G    0.03715     0.0178   0.008838         21        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.872      0.833      0.889      0.453\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n       9/24      2.39G    0.03622     0.0172   0.007886         18        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.914      0.862      0.926       0.49\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      10/24      2.39G     0.0347    0.01694   0.007435         18        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.921      0.888      0.931      0.482\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      11/24      2.39G    0.03402    0.01692   0.006485         26        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.912      0.886      0.935      0.499\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      12/24      2.39G    0.03282    0.01657   0.006057         25        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.934      0.878      0.942      0.507\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      13/24      2.39G    0.03256    0.01694   0.005574         27        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.929      0.916      0.947       0.53\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      14/24      2.39G    0.03148    0.01609   0.005654         22        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.938      0.929      0.961      0.537\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      15/24      2.39G    0.03043    0.01574   0.005036         20        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.941      0.932      0.962      0.544\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      16/24      2.39G    0.02958    0.01528   0.004509         28        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.949      0.938      0.963      0.552\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      17/24      2.39G    0.02913    0.01565   0.004118         18        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.945      0.914      0.957      0.556\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      18/24      2.39G     0.0289    0.01555   0.003923         25        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.953      0.938      0.967      0.569\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      19/24      2.39G    0.02779    0.01504   0.003785         22        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.957      0.942       0.97      0.581\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      20/24      2.39G    0.02695    0.01495   0.003531         31        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.967      0.937      0.971      0.585\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      21/24      2.39G    0.02653    0.01485   0.003323         24        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.955      0.948       0.97      0.598\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      22/24      2.39G    0.02597    0.01459   0.003021         21        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.946      0.941      0.968      0.594\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      23/24      2.39G    0.02549    0.01449   0.002759         32        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.953      0.939      0.966      0.596\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n      24/24      2.39G    0.02489    0.01412   0.002643         21        640: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.962      0.943      0.968      0.606\n\n25 epochs completed in 0.272 hours.\nOptimizer stripped from /kaggle/working/runs/finetune_yolov5/weights/last.pt, 14.4MB\nOptimizer stripped from /kaggle/working/runs/finetune_yolov5/weights/best.pt, 14.4MB\n\nValidating /kaggle/working/runs/finetune_yolov5/weights/best.pt...\nFusing layers... \nModel summary: 157 layers, 7029004 parameters, 0 gradients, 15.8 GFLOPs\n                 Class     Images  Instances          P          R      mAP50   \n                   all        563        802      0.964      0.944      0.969      0.605\n               Grasper        563        299      0.889       0.87       0.92      0.542\n               Bipolar        563         94      0.981      0.915      0.955      0.563\n                  Hook        563         70      0.988      0.971      0.986      0.737\n              Scissors        563         70      0.954      0.971      0.967      0.608\n               Clipper        563         80      0.972       0.95      0.981      0.614\n             Irrigator        563         86      0.976      0.962      0.981      0.566\n           SpecimenBag        563        103      0.987      0.971      0.993      0.605\nResults saved to \u001b[1m/kaggle/working/runs/finetune_yolov5\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ‚ö†Ô∏è wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport torch\nfrom yolov5.utils.general import non_max_suppression\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.dataloaders import LoadImages\nfrom yolov5.utils.torch_utils import select_device\n\n# Ensure all paths have the correct 6-digit format\ntrain_df['frame_path'] = train_df['frame_path'].apply(\n    lambda x: os.path.join(\n        os.path.dirname(x),\n        f\"{int(os.path.basename(x).split('.')[0]):06d}.png\"\n    )\n)\n\n# Validate paths\nprint(\"Validating file paths...\")\nmissing_files = train_df[~train_df['frame_path'].apply(os.path.exists)]\nif not missing_files.empty:\n    print(f\"Warning: {len(missing_files)} files are missing.\")\n    print(missing_files.head())\nelse:\n    print(\"All file paths are valid.\")\n\n# Set model path and device\nmodel_path = \"/kaggle/input/fintune-weight/finetuned.pt\"\ndevice = select_device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load YOLOv5 model\nmodel = DetectMultiBackend(model_path, device=device)\n\n# Function to get bounding boxes for instruments\ndef get_bounding_boxes(image_path):\n    dataset = LoadImages(image_path)\n    results = []\n    \n    for path, img, img0, vid_cap, _ in dataset:\n        img = torch.from_numpy(img).to(device).float() / 255.0  # Normalize\n        if len(img.shape) == 3:\n            img = img.unsqueeze(0)  # Add batch dimension\n        \n        # Perform inference\n        pred = model(img)\n        pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45)\n        \n        for det in pred:\n            if det is not None and len(det):\n                for *box, conf, cls in det:\n                    results.append(box)  # Add bounding box coordinates\n    return results\n\n# Apply YOLOv5 on train_df to get bounding boxes\nbbox_list = []\nfor image_path in tqdm(train_df['frame_path'], desc=\"Processing Images\"):\n    bboxes = get_bounding_boxes(image_path)\n    bbox_list.append(bboxes)\n\n# Save bounding box coordinates in train_df\ntrain_df['bbox'] = bbox_list\n\n# Save train_df with bounding boxes\ntrain_df.to_csv(\"/kaggle/working/train_df_with_bboxes.csv\", index=False)\n\nprint(\"Bounding boxes added to train_df and saved as train_df_with_bboxes.csv.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:13:12.286713Z","iopub.execute_input":"2025-01-12T18:13:12.287106Z","iopub.status.idle":"2025-01-12T19:11:27.317355Z","shell.execute_reply.started":"2025-01-12T18:13:12.287077Z","shell.execute_reply":"2025-01-12T19:11:27.316522Z"}},"outputs":[{"name":"stdout","text":"Validating file paths...\n","output_type":"stream"},{"name":"stderr","text":"YOLOv5 üöÄ v7.0-395-g6420a1db Python-3.10.12 torch-2.4.1+cu121 CPU\n\n","output_type":"stream"},{"name":"stdout","text":"All file paths are valid.\n","output_type":"stream"},{"name":"stderr","text":"Fusing layers... \nModel summary: 157 layers, 7029004 parameters, 0 gradients, 15.8 GFLOPs\nProcessing Images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29253/29253 [57:33<00:00,  8.47it/s] \n","output_type":"stream"},{"name":"stdout","text":"Bounding boxes added to train_df and saved as train_df_with_bboxes.csv.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport torch\n\n# Force environment variable for GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n# Check GPU availability\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:07:52.349364Z","iopub.execute_input":"2025-01-12T18:07:52.349690Z","iopub.status.idle":"2025-01-12T18:07:52.354685Z","shell.execute_reply.started":"2025-01-12T18:07:52.349661Z","shell.execute_reply":"2025-01-12T18:07:52.353827Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:08:20.469072Z","iopub.execute_input":"2025-01-12T18:08:20.469389Z","iopub.status.idle":"2025-01-12T18:08:20.708575Z","shell.execute_reply.started":"2025-01-12T18:08:20.469364Z","shell.execute_reply":"2025-01-12T18:08:20.707679Z"}},"outputs":[{"name":"stdout","text":"Sun Jan 12 18:08:20 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   42C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   42C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}