{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10420483,"sourceType":"datasetVersion","datasetId":6458527},{"sourceId":10421059,"sourceType":"datasetVersion","datasetId":6458944},{"sourceId":10442681,"sourceType":"datasetVersion","datasetId":6463563},{"sourceId":10450853,"sourceType":"datasetVersion","datasetId":6469112},{"sourceId":10451956,"sourceType":"datasetVersion","datasetId":6469923},{"sourceId":10453067,"sourceType":"datasetVersion","datasetId":6470727},{"sourceId":10456821,"sourceType":"datasetVersion","datasetId":6473152},{"sourceId":10456827,"sourceType":"datasetVersion","datasetId":6473157}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install  numpy\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:22:18.622876Z","iopub.execute_input":"2025-01-14T09:22:18.623122Z","iopub.status.idle":"2025-01-14T09:22:22.998201Z","shell.execute_reply.started":"2025-01-14T09:22:18.623102Z","shell.execute_reply":"2025-01-14T09:22:22.997382Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T19:30:21.232388Z","iopub.execute_input":"2025-01-13T19:30:21.232715Z","iopub.status.idle":"2025-01-13T19:30:21.236768Z","shell.execute_reply.started":"2025-01-13T19:30:21.232687Z","shell.execute_reply":"2025-01-13T19:30:21.235740Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T06:02:00.801551Z","iopub.execute_input":"2025-01-13T06:02:00.801868Z","iopub.status.idle":"2025-01-13T06:02:02.363504Z","shell.execute_reply.started":"2025-01-13T06:02:00.801827Z","shell.execute_reply":"2025-01-13T06:02:02.362137Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'yolov5'...\nremote: Enumerating objects: 17129, done.\u001b[K\nremote: Counting objects: 100% (86/86), done.\u001b[K\nremote: Compressing objects: 100% (64/64), done.\u001b[K\nremote: Total 17129 (delta 51), reused 27 (delta 22), pack-reused 17043 (from 2)\u001b[K\nReceiving objects: 100% (17129/17129), 15.80 MiB | 27.56 MiB/s, done.\nResolving deltas: 100% (11750/11750), done.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%cd yolov5\n!pip install -r /kaggle/working/yolov5/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T06:02:04.477360Z","iopub.execute_input":"2025-01-13T06:02:04.477760Z","iopub.status.idle":"2025-01-13T06:02:08.618645Z","shell.execute_reply.started":"2025-01-13T06:02:04.477724Z","shell.execute_reply":"2025-01-13T06:02:08.617161Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/yolov5/yolov5\nRequirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 5)) (3.1.43)\nRequirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 6)) (3.7.1)\nRequirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 7)) (1.26.4)\nRequirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 8)) (4.10.0.84)\nRequirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 9)) (10.4.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 10)) (5.9.5)\nRequirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 11)) (6.0.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 12)) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 13)) (1.13.1)\nRequirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 14)) (0.1.1.post2209072238)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 15)) (2.4.1+cu121)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 16)) (0.19.1+cu121)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 17)) (4.66.5)\nRequirement already satisfied: ultralytics>=8.2.34 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 18)) (8.3.59)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 27)) (2.1.4)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 28)) (0.12.2)\nRequirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/yolov5/requirements.txt (line 42)) (71.0.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython>=3.1.30->-r /kaggle/working/yolov5/requirements.txt (line 5)) (4.0.11)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (1.3.0)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (4.53.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (24.1)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (2.8.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r /kaggle/working/yolov5/requirements.txt (line 12)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r /kaggle/working/yolov5/requirements.txt (line 12)) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r /kaggle/working/yolov5/requirements.txt (line 12)) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r /kaggle/working/yolov5/requirements.txt (line 12)) (2024.8.30)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (2024.6.1)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.2.34->-r /kaggle/working/yolov5/requirements.txt (line 18)) (9.0.0)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.2.34->-r /kaggle/working/yolov5/requirements.txt (line 18)) (2.0.13)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r /kaggle/working/yolov5/requirements.txt (line 27)) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r /kaggle/working/yolov5/requirements.txt (line 27)) (2024.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r /kaggle/working/yolov5/requirements.txt (line 5)) (5.0.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r /kaggle/working/yolov5/requirements.txt (line 6)) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->-r /kaggle/working/yolov5/requirements.txt (line 15)) (1.3.0)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport pytorch_lightning as pl\n\n# Define paths\nvideo_dir = \"/kaggle/input/cholec-train-data/CholecT50/videos\"\nlabel_dir = \"/kaggle/input/cholec-train-data/CholecT50/labels\"\n\n# Training and testing video sub-folder names\ntrain_videos = [\"VID01\", \"VID02\", \"VID04\", \"VID05\", \"VID06\", \"VID08\", \"VID10\", \"VID12\", \"VID13\", \"VID14\"]\ntest_videos = [\"VID92\", \"VID96\", \"VID103\", \"VID110\", \"VID111\"]\n\ndef parse_annotations(label_path):\n    \"\"\"Parse JSON annotations and extract bounding boxes and triplet data.\"\"\"\n    with open(label_path, 'r') as file:\n        data = json.load(file)\n    annotations = data['annotations']\n\n    parsed_data = []\n    for frame_id, triplets in annotations.items():\n        for triplet in triplets:\n            parsed_data.append({\n                'frame_id': int(frame_id),\n                'triplet_id': triplet[0],\n                'instrument_id': triplet[1],\n                'verb_id': triplet[7],\n                'target_id': triplet[8],\n                'phase_id': triplet[14],\n                'bbox': triplet[3:7]\n            })\n    return parsed_data\n\ndef create_dataset(video_list, video_dir, label_dir):\n    \"\"\"Create a DataFrame for the dataset.\"\"\"\n    data = []\n    for video_id in tqdm(video_list, desc=\"Processing Videos\"):\n        video_path = Path(video_dir) / video_id\n        label_path = Path(label_dir) / f\"{video_id}.json\"\n\n        annotations = parse_annotations(label_path)\n        for entry in annotations:\n            frame_path = video_path / f\"{entry['frame_id']}.png\"\n            data.append({\n                'video_id': video_id,\n                'frame_path': str(frame_path),\n                'triplet_id': entry['triplet_id'],\n                'instrument_id': entry['instrument_id'],\n                'verb_id': entry['verb_id'],\n                'target_id': entry['target_id'],\n                'phase_id': entry['phase_id'],\n                'bbox': entry['bbox']\n            })\n    return pd.DataFrame(data)\n\n# Prepare Train and Test DataFrames\ntrain_df = create_dataset(train_videos, video_dir, label_dir)\ntest_df = create_dataset(test_videos, video_dir, label_dir)\n\n# Save DataFrames to CSV for verification\ntrain_df.to_csv(\"train_data.csv\", index=False)\ntest_df.to_csv(\"test_data.csv\", index=False)\n\n# Print DataFrame samples\nprint(\"Training Data Sample:\")\ntrain_df.head()\nprint(\"\\nTesting Data Sample:\")\ntest_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:22:47.008522Z","iopub.execute_input":"2025-01-14T09:22:47.008854Z","iopub.status.idle":"2025-01-14T09:22:54.428420Z","shell.execute_reply.started":"2025-01-14T09:22:47.008827Z","shell.execute_reply":"2025-01-14T09:22:54.427680Z"}},"outputs":[{"name":"stderr","text":"Processing Videos: 100%|██████████| 10/10 [00:00<00:00, 21.60it/s]\nProcessing Videos: 100%|██████████| 5/5 [00:00<00:00, 33.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Data Sample:\n\nTesting Data Sample:\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"  video_id                                         frame_path  triplet_id  \\\n0    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n1    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n2    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n3    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n4    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n\n   instrument_id  verb_id  target_id  phase_id              bbox  \n0             -1       -1         -1         0  [-1, -1, -1, -1]  \n1             -1       -1         -1         0  [-1, -1, -1, -1]  \n2             -1       -1         -1         0  [-1, -1, -1, -1]  \n3             -1       -1         -1         0  [-1, -1, -1, -1]  \n4             -1       -1         -1         0  [-1, -1, -1, -1]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>frame_path</th>\n      <th>triplet_id</th>\n      <th>instrument_id</th>\n      <th>verb_id</th>\n      <th>target_id</th>\n      <th>phase_id</th>\n      <th>bbox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>[-1, -1, -1, -1]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>[-1, -1, -1, -1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>[-1, -1, -1, -1]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>[-1, -1, -1, -1]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>[-1, -1, -1, -1]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:22:57.904580Z","iopub.execute_input":"2025-01-14T09:22:57.904956Z","iopub.status.idle":"2025-01-14T09:22:57.915542Z","shell.execute_reply.started":"2025-01-14T09:22:57.904924Z","shell.execute_reply":"2025-01-14T09:22:57.914890Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"  video_id                                         frame_path  triplet_id  \\\n0    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n1    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n2    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n3    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n4    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n\n   instrument_id  verb_id  target_id  phase_id              bbox  \n0              0        0          0         0  [-1, -1, -1, -1]  \n1              0        0          0         0  [-1, -1, -1, -1]  \n2              0        0          0         0  [-1, -1, -1, -1]  \n3              0        0          0         0  [-1, -1, -1, -1]  \n4              0        0          0         0  [-1, -1, -1, -1]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>frame_path</th>\n      <th>triplet_id</th>\n      <th>instrument_id</th>\n      <th>verb_id</th>\n      <th>target_id</th>\n      <th>phase_id</th>\n      <th>bbox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[-1, -1, -1, -1]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[-1, -1, -1, -1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[-1, -1, -1, -1]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[-1, -1, -1, -1]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[-1, -1, -1, -1]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_df.iloc()[0][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:01:05.996101Z","iopub.execute_input":"2025-01-14T09:01:05.996485Z","iopub.status.idle":"2025-01-14T09:01:06.016449Z","shell.execute_reply.started":"2025-01-14T09:01:05.996442Z","shell.execute_reply":"2025-01-14T09:01:06.015531Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-4-418e02f071fb>:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  train_df.iloc()[0][1]\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/cholec-train-data/CholecT50/videos/VID01/0.png'"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\nimport os\n\nannotations_dir = \"/kaggle/input/finetuning/m2cai16-tool-locations/Annotations\"\nunique_classes = set()\n\nfor annotation_file in os.listdir(annotations_dir)[:10]:  # Check first 10 files\n    annotation_path = os.path.join(annotations_dir, annotation_file)\n    tree = ET.parse(annotation_path)\n    root = tree.getroot()\n\n    for obj in root.findall(\"object\"):\n        class_name = obj.find(\"name\").text\n        unique_classes.add(class_name)\n\nprint(\"Classes found in XML annotations:\")\nprint(unique_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:01:06.018001Z","iopub.execute_input":"2025-01-14T09:01:06.018268Z","iopub.status.idle":"2025-01-14T09:01:06.245765Z","shell.execute_reply.started":"2025-01-14T09:01:06.018245Z","shell.execute_reply":"2025-01-14T09:01:06.244784Z"}},"outputs":[{"name":"stdout","text":"Classes found in XML annotations:\n{'Bipolar', 'Irrigator', 'SpecimenBag', 'Clipper', 'Grasper', 'Scissors'}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# import os\n# import cv2\n# import xml.etree.ElementTree as ET\n# from tqdm import tqdm\n# from sklearn.model_selection import train_test_split\n\n# # Define paths\n# annotations_dir = \"/kaggle/input/finetuning/m2cai16-tool-locations/Annotations\"\n# images_dir = \"/kaggle/input/finetuning/m2cai16-tool-locations/JPEGImages\"\n# class_list_path = \"/kaggle/input/finetuning/m2cai16-tool-locations/class_list.txt\"\n# output_dir = \"/kaggle/working/finetuning_yolo_dataset\"\n\n# # Load and clean class names (strip numeric prefixes)\n# with open(class_list_path, \"r\") as file:\n#     classes = [line.strip().split(' ', 1)[1] for line in file.readlines()]\n\n# print(\"Processed class names from class_list.txt:\")\n# print(classes)\n\n# # Validate class names against XML\n# xml_classes = set()\n# for xml_file in os.listdir(annotations_dir):\n#     tree = ET.parse(os.path.join(annotations_dir, xml_file))\n#     for obj in tree.getroot().findall(\"object\"):\n#         xml_classes.add(obj.find(\"name\").text)\n\n# print(\"Classes found in XML annotations:\")\n# print(xml_classes)\n\n# # Check if all XML classes exist in class_list.txt\n# missing_classes = xml_classes - set(classes)\n# if missing_classes:\n#     print(f\"Warning: Missing classes in class_list.txt: {missing_classes}\")\n# else:\n#     print(\"All XML classes are present in class_list.txt.\")\n\n# # Ensure output directories exist\n# os.makedirs(output_dir, exist_ok=True)\n# os.makedirs(f\"{output_dir}/labels/train\", exist_ok=True)\n# os.makedirs(f\"{output_dir}/labels/val\", exist_ok=True)\n# os.makedirs(f\"{output_dir}/images/train\", exist_ok=True)\n# os.makedirs(f\"{output_dir}/images/val\", exist_ok=True)\n\n# # Function to convert Pascal VOC annotations to YOLO format\n# def voc_to_yolo(annotation_path, image_width, image_height):\n#     tree = ET.parse(annotation_path)\n#     root = tree.getroot()\n#     yolo_annotations = []\n\n#     for obj in root.findall(\"object\"):\n#         class_name = obj.find(\"name\").text\n#         if class_name not in classes:\n#             continue  # Skip unknown classes\n\n#         class_id = classes.index(class_name)\n#         bbox = obj.find(\"bndbox\")\n#         xmin = float(bbox.find(\"xmin\").text)\n#         ymin = float(bbox.find(\"ymin\").text)\n#         xmax = float(bbox.find(\"xmax\").text)\n#         ymax = float(bbox.find(\"ymax\").text)\n\n#         # Convert to YOLO format\n#         x_center = ((xmin + xmax) / 2) / image_width\n#         y_center = ((ymin + ymax) / 2) / image_height\n#         width = (xmax - xmin) / image_width\n#         height = (ymax - ymin) / image_height\n#         yolo_annotations.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n\n#     return yolo_annotations\n\n# # Split data into train and val sets\n# annotation_files = os.listdir(annotations_dir)\n# train_files, val_files = train_test_split(annotation_files, test_size=0.2, random_state=42)\n\n# # Process and save annotations in YOLO format\n# for split, files in [(\"train\", train_files), (\"val\", val_files)]:\n#     for annotation_file in tqdm(files, desc=f\"Processing {split} data\"):\n#         annotation_path = os.path.join(annotations_dir, annotation_file)\n#         image_file = annotation_file.replace(\".xml\", \".jpg\")\n#         image_path = os.path.join(images_dir, image_file)\n        \n#         # Get image dimensions\n#         if not os.path.exists(image_path):\n#             print(f\"Image not found: {image_path}\")\n#             continue\n        \n#         image = cv2.imread(image_path)\n#         height, width, _ = image.shape\n\n#         # Convert annotations\n#         yolo_annotations = voc_to_yolo(annotation_path, width, height)\n        \n#         # Save annotations\n#         with open(f\"{output_dir}/labels/{split}/{image_file.replace('.jpg', '.txt')}\", \"w\") as file:\n#             file.write(\"\\n\".join(yolo_annotations))\n        \n#         # Copy images to the corresponding directory\n#         os.system(f\"cp {image_path} {output_dir}/images/{split}/\")\n\n# # Save dataset.yaml for YOLOv5 training\n# dataset_yaml = f\"\"\"\n# train: {output_dir}/images/train\n# val: {output_dir}/images/val\n\n# nc: {len(classes)}\n# names: {classes}\n# \"\"\"\n# with open(f\"{output_dir}/dataset.yaml\", \"w\") as file:\n#     file.write(dataset_yaml)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:01:06.246836Z","iopub.execute_input":"2025-01-14T09:01:06.247158Z","iopub.status.idle":"2025-01-14T09:01:06.252460Z","shell.execute_reply.started":"2025-01-14T09:01:06.247132Z","shell.execute_reply":"2025-01-14T09:01:06.251191Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import os\n\n# Disable W&B visualization by setting the environment variable\nos.environ[\"WANDB_MODE\"] = \"disabled\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:01:06.253324Z","iopub.execute_input":"2025-01-14T09:01:06.253665Z","iopub.status.idle":"2025-01-14T09:01:06.272898Z","shell.execute_reply.started":"2025-01-14T09:01:06.253637Z","shell.execute_reply":"2025-01-14T09:01:06.271670Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# !rm /kaggle/working/finetuning_yolo_dataset/labels/train.cache\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:01:06.273968Z","iopub.execute_input":"2025-01-14T09:01:06.274282Z","iopub.status.idle":"2025-01-14T09:01:06.286895Z","shell.execute_reply.started":"2025-01-14T09:01:06.274250Z","shell.execute_reply":"2025-01-14T09:01:06.285725Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# !python train.py --img 640 --batch 16 --epochs 25 --data /kaggle/working/finetuning_yolo_dataset/dataset.yaml --weights /kaggle/input/model-weight-1/best.pt --project /kaggle/working/runs --name finetune_yolov5\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:01:06.288925Z","iopub.execute_input":"2025-01-14T09:01:06.289233Z","iopub.status.idle":"2025-01-14T09:01:06.301872Z","shell.execute_reply.started":"2025-01-14T09:01:06.289205Z","shell.execute_reply":"2025-01-14T09:01:06.300638Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport torch\nfrom yolov5.utils.general import non_max_suppression\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.dataloaders import LoadImages\nfrom yolov5.utils.torch_utils import select_device\n\n# Ensure all paths have the correct 6-digit format\ntrain_df['frame_path'] = train_df['frame_path'].apply(\n    lambda x: os.path.join(\n        os.path.dirname(x),\n        f\"{int(os.path.basename(x).split('.')[0]):06d}.png\"\n    )\n)\n\n# Validate paths\nprint(\"Validating file paths...\")\nmissing_files = train_df[~train_df['frame_path'].apply(os.path.exists)]\nif not missing_files.empty:\n    print(f\"Warning: {len(missing_files)} files are missing.\")\n    print(missing_files.head())\nelse:\n    print(\"All file paths are valid.\")\n\n# Set model path and device\nmodel_path = \"/kaggle/input/fintune-weight/finetuned.pt\"\ndevice = select_device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load YOLOv5 model\nmodel = DetectMultiBackend(model_path, device=device)\n\n# Function to get bounding boxes for instruments\ndef get_bounding_boxes(image_path):\n    dataset = LoadImages(image_path)\n    results = []\n    \n    for path, img, img0, vid_cap, _ in dataset:\n        img = torch.from_numpy(img).to(device).float() / 255.0  # Normalize\n        if len(img.shape) == 3:\n            img = img.unsqueeze(0)  # Add batch dimension\n        \n        # Perform inference\n        pred = model(img)\n        pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45)\n        \n        for det in pred:\n            if det is not None and len(det):\n                for *box, conf, cls in det:\n                    results.append(box)  # Add bounding box coordinates\n    return results\n\n# Apply YOLOv5 on train_df to get bounding boxes\nbbox_list = []\nfor image_path in tqdm(train_df['frame_path'], desc=\"Processing Images\"):\n    bboxes = get_bounding_boxes(image_path)\n    bbox_list.append(bboxes)\n\n# Save bounding box coordinates in train_df\ntrain_df['bbox'] = bbox_list\n\n# Save train_df with bounding boxes\ntrain_df.to_csv(\"/kaggle/working/train_df_with_bboxes.csv\", index=False)\n\nprint(\"Bounding boxes added to train_df and saved as train_df_with_bboxes.csv.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:01:06.302961Z","iopub.execute_input":"2025-01-14T09:01:06.303295Z","iopub.status.idle":"2025-01-14T09:01:06.496298Z","shell.execute_reply.started":"2025-01-14T09:01:06.303268Z","shell.execute_reply":"2025-01-14T09:01:06.494934Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-5eccf6fcce9e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myolov5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnon_max_suppression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myolov5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetectMultiBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myolov5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoadImages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yolov5'"],"ename":"ModuleNotFoundError","evalue":"No module named 'yolov5'","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport torch\nfrom yolov5.utils.general import non_max_suppression\nfrom yolov5.models.common import DetectMultiBackend\nfrom yolov5.utils.dataloaders import LoadImages\nfrom yolov5.utils.torch_utils import select_device\n\n# Ensure all paths in test_df have the correct 6-digit format\ntest_df['frame_path'] = test_df['frame_path'].apply(\n    lambda x: os.path.join(\n        os.path.dirname(x),\n        f\"{int(os.path.basename(x).split('.')[0]):06d}.png\"\n    )\n)\n\n# Validate paths in test_df\nprint(\"Validating file paths in test_df...\")\nmissing_files_test = test_df[~test_df['frame_path'].apply(os.path.exists)]\nif not missing_files_test.empty:\n    print(f\"Warning: {len(missing_files_test)} files are missing in test_df.\")\n    print(missing_files_test.head())\nelse:\n    print(\"All file paths in test_df are valid.\")\n\n# Set model path and device\nmodel_path = \"/kaggle/input/fintune-weight/finetuned.pt\"\ndevice = select_device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load YOLOv5 model\nmodel = DetectMultiBackend(model_path, device=device)\n\n# Function to get bounding boxes for instruments\ndef get_bounding_boxes(image_path):\n    dataset = LoadImages(image_path)\n    results = []\n    \n    for path, img, img0, vid_cap, _ in dataset:\n        img = torch.from_numpy(img).to(device).float() / 255.0  # Normalize\n        if len(img.shape) == 3:\n            img = img.unsqueeze(0)  # Add batch dimension\n        \n        # Perform inference\n        pred = model(img)\n        pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45)\n        \n        for det in pred:\n            if det is not None and len(det):\n                for *box, conf, cls in det:\n                    results.append(box)  # Add bounding box coordinates\n    return results\n\n# Apply YOLOv5 on test_df to get bounding boxes\nbbox_list_test = []\nfor image_path in tqdm(test_df['frame_path'], desc=\"Processing Test Images\"):\n    bboxes = get_bounding_boxes(image_path)\n    bbox_list_test.append(bboxes)\n\n# Save bounding box coordinates in test_df\ntest_df['bbox'] = bbox_list_test\n\n# Save test_df with bounding boxes\ntest_df.to_csv(\"/kaggle/working/test_df_with_bboxes.csv\", index=False)\n\nprint(\"Bounding boxes added to test_df and saved as test_df_with_bboxes.csv.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:01:06.497105Z","iopub.status.idle":"2025-01-14T09:01:06.497546Z","shell.execute_reply":"2025-01-14T09:01:06.497343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Specify the path to the CSV file\ncsv_file_path = \"/kaggle/input/procesed-train-data/train_df_with_bboxes.csv\"\n\n# Load the CSV file into a DataFrame\ntrain_df_m = pd.read_csv(csv_file_path)\n\n# Display the first few rows of the DataFrame to confirm it's loaded\ntrain_df_m.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:23:11.833213Z","iopub.execute_input":"2025-01-14T09:23:11.833500Z","iopub.status.idle":"2025-01-14T09:23:11.976818Z","shell.execute_reply.started":"2025-01-14T09:23:11.833481Z","shell.execute_reply":"2025-01-14T09:23:11.976032Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"  video_id                                         frame_path  triplet_id  \\\n0    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n1    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n2    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n3    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n4    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n\n   instrument_id  verb_id  target_id  phase_id  \\\n0              0        0        1.0         0   \n1              0        0        1.0         0   \n2              0        0        1.0         0   \n3              0        0        1.0         0   \n4              0        0        1.0         0   \n\n                                                bbox  \n0  [[tensor(285.87274), tensor(16.54350), tensor(...  \n1  [[tensor(179.74133), tensor(19.72769), tensor(...  \n2  [[tensor(140.20927), tensor(19.87531), tensor(...  \n3                                                 []  \n4                                                 []  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>frame_path</th>\n      <th>triplet_id</th>\n      <th>instrument_id</th>\n      <th>verb_id</th>\n      <th>target_id</th>\n      <th>phase_id</th>\n      <th>bbox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>[[tensor(285.87274), tensor(16.54350), tensor(...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>[[tensor(179.74133), tensor(19.72769), tensor(...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>[[tensor(140.20927), tensor(19.87531), tensor(...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Copy the 'bbox' column from train_df_m to train_df\ntrain_df['bbox'] = train_df_m['bbox']\n\n# Save train_df to a new file (e.g., as a CSV)\n# train_df.to_csv('train_df_updated.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:23:15.005896Z","iopub.execute_input":"2025-01-14T09:23:15.006182Z","iopub.status.idle":"2025-01-14T09:23:15.011988Z","shell.execute_reply.started":"2025-01-14T09:23:15.006161Z","shell.execute_reply":"2025-01-14T09:23:15.011172Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:23:16.988273Z","iopub.execute_input":"2025-01-14T09:23:16.988533Z","iopub.status.idle":"2025-01-14T09:23:16.997399Z","shell.execute_reply.started":"2025-01-14T09:23:16.988512Z","shell.execute_reply":"2025-01-14T09:23:16.996580Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"  video_id                                         frame_path  triplet_id  \\\n0    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n1    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n2    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n3    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n4    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n\n   instrument_id  verb_id  target_id  phase_id  \\\n0              0        0          0         0   \n1              0        0          0         0   \n2              0        0          0         0   \n3              0        0          0         0   \n4              0        0          0         0   \n\n                                                bbox  \n0  [[tensor(285.87274), tensor(16.54350), tensor(...  \n1  [[tensor(179.74133), tensor(19.72769), tensor(...  \n2  [[tensor(140.20927), tensor(19.87531), tensor(...  \n3                                                 []  \n4                                                 []  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>frame_path</th>\n      <th>triplet_id</th>\n      <th>instrument_id</th>\n      <th>verb_id</th>\n      <th>target_id</th>\n      <th>phase_id</th>\n      <th>bbox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[[tensor(285.87274), tensor(16.54350), tensor(...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[[tensor(179.74133), tensor(19.72769), tensor(...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[[tensor(140.20927), tensor(19.87531), tensor(...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"print(train_df.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:23:19.085394Z","iopub.execute_input":"2025-01-14T09:23:19.085729Z","iopub.status.idle":"2025-01-14T09:23:19.091141Z","shell.execute_reply.started":"2025-01-14T09:23:19.085683Z","shell.execute_reply":"2025-01-14T09:23:19.090167Z"}},"outputs":[{"name":"stdout","text":"Index(['video_id', 'frame_path', 'triplet_id', 'instrument_id', 'verb_id',\n       'target_id', 'phase_id', 'bbox'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\n\n# Specify the path to the CSV file\ncsv_file_path = \"/kaggle/input/procesed-test-data/test_df_with_bboxes.csv\"\n\n# Load the CSV file into a DataFrame\ntest_df_m = pd.read_csv(csv_file_path)\n\n# Display the first few rows of the DataFrame to confirm it's loaded\ntest_df_m.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:23:21.217334Z","iopub.execute_input":"2025-01-14T09:23:21.217610Z","iopub.status.idle":"2025-01-14T09:23:21.287720Z","shell.execute_reply.started":"2025-01-14T09:23:21.217590Z","shell.execute_reply":"2025-01-14T09:23:21.287026Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"  video_id                                         frame_path  triplet_id  \\\n0    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n1    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n2    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n3    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n4    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n\n   instrument_id  verb_id  target_id  phase_id bbox  \n0             -1       -1        1.0         0   []  \n1             -1       -1        1.0         0   []  \n2             -1       -1        1.0         0   []  \n3             -1       -1        1.0         0   []  \n4             -1       -1        1.0         0   []  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>frame_path</th>\n      <th>triplet_id</th>\n      <th>instrument_id</th>\n      <th>verb_id</th>\n      <th>target_id</th>\n      <th>phase_id</th>\n      <th>bbox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Copy the 'bbox' column from train_df_m to train_df\ntest_df['bbox'] = test_df_m['bbox']\n\n# Save train_df to a new file (e.g., as a CSV)\n# test_df.to_csv('test_df_updated.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:23:25.358892Z","iopub.execute_input":"2025-01-14T09:23:25.359178Z","iopub.status.idle":"2025-01-14T09:23:25.363925Z","shell.execute_reply.started":"2025-01-14T09:23:25.359157Z","shell.execute_reply":"2025-01-14T09:23:25.362952Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"   test_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:23:27.770518Z","iopub.execute_input":"2025-01-14T09:23:27.770850Z","iopub.status.idle":"2025-01-14T09:23:27.780323Z","shell.execute_reply.started":"2025-01-14T09:23:27.770823Z","shell.execute_reply":"2025-01-14T09:23:27.779678Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"  video_id                                         frame_path  triplet_id  \\\n0    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n1    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n2    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n3    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n4    VID92  /kaggle/input/cholec-train-data/CholecT50/vide...          -1   \n\n   instrument_id  verb_id  target_id  phase_id bbox  \n0             -1       -1         -1         0   []  \n1             -1       -1         -1         0   []  \n2             -1       -1         -1         0   []  \n3             -1       -1         -1         0   []  \n4             -1       -1         -1         0   []  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>frame_path</th>\n      <th>triplet_id</th>\n      <th>instrument_id</th>\n      <th>verb_id</th>\n      <th>target_id</th>\n      <th>phase_id</th>\n      <th>bbox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>VID92</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"train_df.iloc[0][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:03:07.123405Z","iopub.execute_input":"2025-01-14T09:03:07.123781Z","iopub.status.idle":"2025-01-14T09:03:07.131176Z","shell.execute_reply.started":"2025-01-14T09:03:07.123752Z","shell.execute_reply":"2025-01-14T09:03:07.129735Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-18-3841664e9223>:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  train_df.iloc[0][1]\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/cholec-train-data/CholecT50/videos/VID01/0.png'"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"train_df['target_id'].nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:23:31.007659Z","iopub.execute_input":"2025-01-14T09:23:31.007959Z","iopub.status.idle":"2025-01-14T09:23:31.016277Z","shell.execute_reply.started":"2025-01-14T09:23:31.007938Z","shell.execute_reply":"2025-01-14T09:23:31.015605Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"16"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"test_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:17:05.661387Z","iopub.execute_input":"2025-01-14T10:17:05.661722Z","iopub.status.idle":"2025-01-14T10:17:05.666937Z","shell.execute_reply.started":"2025-01-14T10:17:05.661670Z","shell.execute_reply":"2025-01-14T10:17:05.666217Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"Index(['video_id', 'frame_path', 'triplet_id', 'instrument_id', 'verb_id',\n       'target_id', 'phase_id', 'bbox'],\n      dtype='object')"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from torchvision.models import resnet50\n# import pytorch_lightning as pl\n# import os\n\n# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # For debugging CUDA errors\n# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n\n# # Load classes from file\n# def load_classes_from_file(file_path):\n#     with open(file_path, 'r') as file:\n#         classes = file.read().splitlines()  # Read all lines and remove newlines\n#     return classes\n\n# # Paths to the files containing the class mappings\n# instrument_classes_path = '/kaggle/input/dict-mapping/dict/instrument.txt'\n# target_classes_path = '/kaggle/input/dict-mapping/dict/target.txt'\n# verb_classes_path = '/kaggle/input/dict-mapping/dict/verb.txt'\n# triplet_classes_path = '/kaggle/input/dict-mapping/dict/triplet.txt'\n\n# # Load classes from the files\n# instrument_classes = load_classes_from_file(instrument_classes_path)\n# target_classes = load_classes_from_file(target_classes_path)\n# verb_classes = load_classes_from_file(verb_classes_path)\n# triplet_classes = load_classes_from_file(triplet_classes_path)\n\n# # Define MultiTaskTripletModel\n# class MultiTaskTripletModel(nn.Module):\n#     def __init__(self, num_instruments, num_verbs, num_targets, num_triplets):\n#         super(MultiTaskTripletModel, self).__init__()\n\n#         # Load a pre-trained ResNet-50 backbone\n#         self.backbone = resnet50(pretrained=True)\n#         self.backbone.fc = nn.Identity()  # Remove the fully connected layer\n\n#         # Fully connected layers for each task\n#         self.instrument_head = nn.Sequential(\n#             nn.Linear(2048, 512),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),\n#             nn.Linear(512, num_instruments)\n#         )\n\n#         self.verb_head = nn.Sequential(\n#             nn.Linear(2048, 512),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),\n#             nn.Linear(512, num_verbs)\n#         )\n\n#         self.target_head = nn.Sequential(\n#             nn.Linear(2048, 512),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),\n#             nn.Linear(512, num_targets)\n#         )\n\n#         self.triplet_head = nn.Sequential(\n#             nn.Linear(2048, 512),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),\n#             nn.Linear(512, num_triplets)\n#         )\n\n#     def forward(self, x):\n#         features = self.backbone(x)  # Extract features from the backbone\n\n#         instrument_logits = self.instrument_head(features)\n#         verb_logits = self.verb_head(features)\n#         target_logits = self.target_head(features)\n#         triplet_logits = self.triplet_head(features)\n\n#         return instrument_logits, verb_logits, target_logits, triplet_logits\n\n# # Define MultiTaskTripletLightningModel\n# class MultiTaskTripletLightningModel(pl.LightningModule):\n#     def __init__(self, num_instruments, num_verbs, num_targets, num_triplets):\n#         super(MultiTaskTripletLightningModel, self).__init__()\n#         self.model = MultiTaskTripletModel(num_instruments, num_verbs, num_targets, num_triplets)\n#         self.criterion = nn.CrossEntropyLoss()\n\n#     def forward(self, x):\n#         return self.model(x)\n\n#     def training_step(self, batch, batch_idx):\n#         images, labels_dict, _ = batch  # Unpack batch\n#         instrument_logits, verb_logits, target_logits, triplet_logits = self(images)\n\n#         # Compute losses\n#         instrument_loss = self.criterion(instrument_logits, labels_dict[\"instrument_id\"])\n#         verb_loss = self.criterion(verb_logits, labels_dict[\"verb_id\"])\n#         target_loss = self.criterion(target_logits, labels_dict[\"target_id\"])\n#         triplet_loss = self.criterion(triplet_logits, labels_dict[\"triplet_id\"])\n\n#         # Total loss\n#         total_loss = instrument_loss + verb_loss + target_loss + triplet_loss\n#         self.log(\"train_loss\", total_loss)\n\n#         return total_loss\n\n#     def validation_step(self, batch, batch_idx):\n#         images, labels_dict, _ = batch\n#         instrument_logits, verb_logits, target_logits, triplet_logits = self(images)\n        \n#         # Ensure logits are float32\n#         instrument_logits = instrument_logits.float()\n#         verb_logits = verb_logits.float()\n#         target_logits = target_logits.float()\n#         triplet_logits = triplet_logits.float()\n\n#         # Ensure the labels are long (int64) type\n#         instrument_labels = labels_dict[\"instrument_id\"].long()\n#         verb_labels = labels_dict[\"verb_id\"].long()\n#         target_labels = labels_dict[\"target_id\"].long()\n#         triplet_labels = labels_dict[\"triplet_id\"].long()\n\n#         # Compute losses\n#         loss_instrument = self.criterion(instrument_logits, instrument_labels)\n#         loss_verb = self.criterion(verb_logits, verb_labels)\n#         loss_target = self.criterion(target_logits, target_labels)\n#         loss_triplet = self.criterion(triplet_logits, triplet_labels)\n\n#         # Total loss\n#         total_loss = loss_instrument + loss_verb + loss_target + loss_triplet\n#         self.log(\"val_loss\", total_loss)\n\n#         return total_loss\n\n#     def configure_optimizers(self):\n#         return torch.optim.Adam(self.parameters(), lr=1e-3)\n\n\n# # # Trainer Setup\n# # trainer = pl.Trainer(max_epochs=10, gpus=1)  # Add GPU if available\n\n# # # Fit Model\n# # trainer.fit(lightning_model, datamodule=data_module)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T12:59:20.071288Z","iopub.execute_input":"2025-01-13T12:59:20.071573Z","iopub.status.idle":"2025-01-13T12:59:20.086354Z","shell.execute_reply.started":"2025-01-13T12:59:20.071552Z","shell.execute_reply":"2025-01-13T12:59:20.085458Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"import torch\ndef preprocess_bbox_column(dataframe):\n    \"\"\"\n    Converts the bbox column in the DataFrame from strings to lists of tensors.\n\n    Args:\n        dataframe (pd.DataFrame): DataFrame with a 'bbox' column.\n\n    Returns:\n        pd.DataFrame: DataFrame with the bbox column preprocessed.\n    \"\"\"\n    def parse_bbox(bbox_str):\n        if isinstance(bbox_str, str):\n            try:\n                # Parse the string representation of bbox manually\n                bbox_list = eval(bbox_str.replace(\"tensor(\", \"\").replace(\")\", \"\"))\n                # Convert all items in the list to tensors\n                return [torch.tensor(b, dtype=torch.float32) for b in bbox_list]\n            except Exception as e:\n                raise ValueError(f\"Error parsing bbox: {bbox_str}. Error: {e}\")\n        elif isinstance(bbox_str, list):\n            # Ensure all elements are tensors\n            return [torch.tensor(b, dtype=torch.float32) for b in bbox_str]\n        else:\n            return []  # Handle empty bbox\n\n    dataframe[\"bbox\"] = dataframe[\"bbox\"].apply(parse_bbox)\n    return dataframe\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:25:06.200361Z","iopub.execute_input":"2025-01-14T09:25:06.200661Z","iopub.status.idle":"2025-01-14T09:25:06.206069Z","shell.execute_reply.started":"2025-01-14T09:25:06.200639Z","shell.execute_reply":"2025-01-14T09:25:06.205271Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_df = preprocess_bbox_column(train_df)\ntest_df = preprocess_bbox_column(test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:25:09.841278Z","iopub.execute_input":"2025-01-14T09:25:09.841550Z","iopub.status.idle":"2025-01-14T09:25:11.067449Z","shell.execute_reply.started":"2025-01-14T09:25:09.841530Z","shell.execute_reply":"2025-01-14T09:25:11.066777Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"train_df['verb_id'] = train_df['verb_id'].replace(-1, 9)\ntrain_df['target_id'] = train_df['target_id'].replace(-1, 14)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:25:16.433439Z","iopub.execute_input":"2025-01-14T09:25:16.433733Z","iopub.status.idle":"2025-01-14T09:25:16.439452Z","shell.execute_reply.started":"2025-01-14T09:25:16.433711Z","shell.execute_reply":"2025-01-14T09:25:16.438781Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"train_df['target_id'].nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:25:18.756252Z","iopub.execute_input":"2025-01-14T09:25:18.756511Z","iopub.status.idle":"2025-01-14T09:25:18.761874Z","shell.execute_reply.started":"2025-01-14T09:25:18.756491Z","shell.execute_reply":"2025-01-14T09:25:18.761176Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"15"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:25:22.360796Z","iopub.execute_input":"2025-01-14T09:25:22.361235Z","iopub.status.idle":"2025-01-14T09:25:22.366098Z","shell.execute_reply.started":"2025-01-14T09:25:22.361197Z","shell.execute_reply":"2025-01-14T09:25:22.364936Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df= train_df[:100]\ntest_df= test_df[:100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:25:27.438970Z","iopub.execute_input":"2025-01-14T09:25:27.439244Z","iopub.status.idle":"2025-01-14T09:25:27.442965Z","shell.execute_reply.started":"2025-01-14T09:25:27.439224Z","shell.execute_reply":"2025-01-14T09:25:27.442106Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:25:29.653067Z","iopub.execute_input":"2025-01-14T09:25:29.653358Z","iopub.status.idle":"2025-01-14T09:25:29.726404Z","shell.execute_reply.started":"2025-01-14T09:25:29.653336Z","shell.execute_reply":"2025-01-14T09:25:29.725792Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"  video_id                                         frame_path  triplet_id  \\\n0    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n1    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n2    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n3    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n4    VID01  /kaggle/input/cholec-train-data/CholecT50/vide...           7   \n\n   instrument_id  verb_id  target_id  phase_id  \\\n0              0        0          0         0   \n1              0        0          0         0   \n2              0        0          0         0   \n3              0        0          0         0   \n4              0        0          0         0   \n\n                                                bbox  \n0  [[tensor(285.8727), tensor(16.5435), tensor(38...  \n1  [[tensor(179.7413), tensor(19.7277), tensor(31...  \n2  [[tensor(140.2093), tensor(19.8753), tensor(27...  \n3                                                 []  \n4                                                 []  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>frame_path</th>\n      <th>triplet_id</th>\n      <th>instrument_id</th>\n      <th>verb_id</th>\n      <th>target_id</th>\n      <th>phase_id</th>\n      <th>bbox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[[tensor(285.8727), tensor(16.5435), tensor(38...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[[tensor(179.7413), tensor(19.7277), tensor(31...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[[tensor(140.2093), tensor(19.8753), tensor(27...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>VID01</td>\n      <td>/kaggle/input/cholec-train-data/CholecT50/vide...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"train_df.iloc[0][7]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:04:50.579448Z","iopub.execute_input":"2025-01-14T09:04:50.579799Z","iopub.status.idle":"2025-01-14T09:04:50.587778Z","shell.execute_reply.started":"2025-01-14T09:04:50.579771Z","shell.execute_reply":"2025-01-14T09:04:50.586982Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-29-c081e571bf05>:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  train_df.iloc[0][7]\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"[tensor([285.8727,  16.5435, 381.9036,  66.8849])]"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"# Define the input and output file paths\ninput_file_path = '/kaggle/input/dict-mapping/dict/instrument.txt'\noutput_file_path = '/kaggle/working/instrument_updated.txt'\n\n# Open the input file and read its contents\nwith open(input_file_path, 'r') as file:\n    # Read the file content as a string\n    data = file.read()\n\n# Convert the file content into a dictionary (since the format appears dictionary-like)\ninstrument_dict = {\n    0: 'grasper',\n    1: 'bipolar',\n    2: 'hook',\n    3: 'scissors',\n    4: 'clipper',\n    5: 'irrigator'\n}\n\n# Add the new class to the dictionary\ninstrument_dict[6] = 'null_instrument'\n\n# Now, save the updated dictionary to a new file in /kaggle/working\nwith open(output_file_path, 'w') as file:\n    # Convert the dictionary into a string and write it back to the file\n    for key, value in instrument_dict.items():\n        file.write(f\"{key}:{value}\\n\")\n\nprint(f\"New class '6: null_instrument' added and file saved as {output_file_path}!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:25:34.694133Z","iopub.execute_input":"2025-01-14T09:25:34.694403Z","iopub.status.idle":"2025-01-14T09:25:34.706929Z","shell.execute_reply.started":"2025-01-14T09:25:34.694384Z","shell.execute_reply":"2025-01-14T09:25:34.706179Z"}},"outputs":[{"name":"stdout","text":"New class '6: null_instrument' added and file saved as /kaggle/working/instrument_updated.txt!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Define the input and output file paths\ninput_file_path = '/kaggle/input/dict-mapping/dict/triplet.txt'\noutput_file_path = '/kaggle/working/triplet_updated.txt'\n\n# Open the input file and read its contents\nwith open(input_file_path, 'r') as file:\n    # Read the file content as a string\n    data = file.readlines()\n\n# Add the new class to the data\nnew_class = \"100:null_instrument,null_verb,null_target\\n\"\ndata.append(new_class)\n\n# Now, save the updated content to the new file in /kaggle/working\nwith open(output_file_path, 'w') as file:\n    file.writelines(data)\n\nprint(f\"New class '100:null_instrument,null_verb,null_target' added and file saved as {output_file_path}!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:25:38.469928Z","iopub.execute_input":"2025-01-14T09:25:38.470242Z","iopub.status.idle":"2025-01-14T09:25:38.479122Z","shell.execute_reply.started":"2025-01-14T09:25:38.470215Z","shell.execute_reply":"2025-01-14T09:25:38.478295Z"}},"outputs":[{"name":"stdout","text":"New class '100:null_instrument,null_verb,null_target' added and file saved as /kaggle/working/triplet_updated.txt!\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Concatenate, Input\nfrom tensorflow.keras.models import Model\nimport pandas as pd\nfrom PIL import Image\nimport numpy as np\nimport os\n\n# # Disable GPU execution\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n# tf.config.set_visible_devices([], 'GPU')  # Disables all GPUs\n# Ensure TensorFlow GPU setup is done before any operations\nphysical_devices = tf.config.list_physical_devices('GPU')\nif physical_devices:\n    # Set visible device to first GPU\n    tf.config.set_visible_devices(physical_devices[0], 'GPU')\n    # Enable memory growth for the first GPU\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Set device for torch as well\n    print(\"GPU detected and memory growth enabled.\")\nelse:\n    print(\"No GPU detected, using CPU.\")\n    device = torch.device('cpu')\n\n# Load class mappings\ndef load_classes_from_file(file_path):\n    with open(file_path, 'r') as file:\n        classes = file.read().splitlines()\n    return classes\n\ninstrument_classes = load_classes_from_file('/kaggle/working/instrument_updated.txt')\ntarget_classes = load_classes_from_file('/kaggle/input/dict-mapping/dict/target.txt')\nverb_classes = load_classes_from_file('/kaggle/input/dict-mapping/dict/verb.txt')\ntriplet_classes = load_classes_from_file('/kaggle/working/triplet_updated.txt')\n# Data loading and preprocessing with bbox handling","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:26:19.803440Z","iopub.execute_input":"2025-01-14T09:26:19.803789Z","iopub.status.idle":"2025-01-14T09:26:26.950295Z","shell.execute_reply.started":"2025-01-14T09:26:19.803752Z","shell.execute_reply":"2025-01-14T09:26:26.949558Z"}},"outputs":[{"name":"stdout","text":"GPU detected and memory growth enabled.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"!pip install --upgrade tensorflow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:47:31.160685Z","iopub.execute_input":"2025-01-14T09:47:31.161069Z","iopub.status.idle":"2025-01-14T09:48:31.652343Z","shell.execute_reply.started":"2025-01-14T09:47:31.161040Z","shell.execute_reply":"2025-01-14T09:48:31.651256Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\nCollecting tensorflow\n  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\nCollecting tensorboard<2.19,>=2.18 (from tensorflow)\n  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting keras>=3.5.0 (from tensorflow)\n  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.8.1)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.12.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\nDownloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tensorboard, keras, tensorflow\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.17.0\n    Uninstalling tensorboard-2.17.0:\n      Successfully uninstalled tensorboard-2.17.0\n  Attempting uninstall: keras\n    Found existing installation: keras 3.4.1\n    Uninstalling keras-3.4.1:\n      Successfully uninstalled keras-3.4.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.17.0\n    Uninstalling tensorflow-2.17.0:\n      Successfully uninstalled tensorflow-2.17.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.18.0 which is incompatible.\ntensorflow-text 2.17.0 requires tensorflow<2.18,>=2.17.0, but you have tensorflow 2.18.0 which is incompatible.\ntf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-3.8.0 tensorboard-2.18.0 tensorflow-2.18.0\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import os\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:41:57.277532Z","iopub.execute_input":"2025-01-14T09:41:57.277871Z","iopub.status.idle":"2025-01-14T09:41:57.281491Z","shell.execute_reply.started":"2025-01-14T09:41:57.277841Z","shell.execute_reply":"2025-01-14T09:41:57.280793Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"physical_devices = tf.config.list_physical_devices('GPU')\nif physical_devices:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:38:53.718980Z","iopub.execute_input":"2025-01-14T09:38:53.719271Z","iopub.status.idle":"2025-01-14T09:38:53.723293Z","shell.execute_reply.started":"2025-01-14T09:38:53.719249Z","shell.execute_reply":"2025-01-14T09:38:53.722517Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:39:47.134611Z","iopub.execute_input":"2025-01-14T09:39:47.134964Z","iopub.status.idle":"2025-01-14T09:39:47.139629Z","shell.execute_reply.started":"2025-01-14T09:39:47.134938Z","shell.execute_reply":"2025-01-14T09:39:47.138803Z"}},"outputs":[{"name":"stdout","text":"Num GPUs Available:  2\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"test_df.iloc[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:12:25.212773Z","iopub.execute_input":"2025-01-14T10:12:25.213056Z","iopub.status.idle":"2025-01-14T10:12:25.219458Z","shell.execute_reply.started":"2025-01-14T10:12:25.213035Z","shell.execute_reply":"2025-01-14T10:12:25.218520Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"video_id                                                     VID92\nframe_path       /kaggle/input/cholec-train-data/CholecT50/vide...\ntriplet_id                                                      -1\ninstrument_id                                                   -1\nverb_id                                                         -1\ntarget_id                                                       -1\nphase_id                                                         0\nbbox                                                            []\nName: 0, dtype: object"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"import tensorflow as tf\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"CUDA version: {tf.version.COMPILER_VERSION}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T09:35:40.626593Z","iopub.execute_input":"2025-01-14T09:35:40.626964Z","iopub.status.idle":"2025-01-14T09:35:40.631559Z","shell.execute_reply.started":"2025-01-14T09:35:40.626933Z","shell.execute_reply":"2025-01-14T09:35:40.630852Z"}},"outputs":[{"name":"stdout","text":"TensorFlow version: 2.17.0\nCUDA version: Ubuntu Clang 17.0.6 (++20231208085846+6009708b4367-1~exp1~20231208085949.74)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Concatenate, Lambda\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import Callback\nfrom PIL import Image\nfrom tensorflow.keras.layers import Resizing\nimport torch\nfrom tensorflow.keras.layers import Layer\ntf.debugging.set_log_device_placement(True)\n\n# Preprocess image function\ndef preprocess_image(image_path, label, num_instruments, num_verbs, num_targets, num_triplets):\n    \"\"\"\n    Load and preprocess the image, ensuring proper formatting, resizing, normalization, \n    and handling of missing triplet IDs.\n    \"\"\"\n    filename = os.path.basename(image_path)\n    image_id = int(filename.split('.')[0])\n    padded_image_id = f\"{image_id:06d}\"  # Ensure zero-padded\n    corrected_image_path = os.path.join(os.path.dirname(image_path), f\"{padded_image_id}.png\")\n\n    # Load and preprocess the image\n    try:\n        # Load the image using PIL\n        image = Image.open(corrected_image_path).convert(\"RGB\")\n        \n        # Resize the image to 224x224 using LANCZOS\n        image = image.resize((224, 224), Image.LANCZOS)\n        \n        # Convert to numpy array and normalize\n        image = np.array(image, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n    except FileNotFoundError:\n        print(f\"Warning: Image not found: {corrected_image_path}\")\n        # Create a blank 224x224 RGB image as fallback\n        image = np.zeros((224, 224, 3), dtype=np.float32)\n\n    # Handle bounding boxes (ensure they are within [0, 1])\n    bbox = label.get(\"bbox\", [0.0, 0.0, 1.0, 1.0])  # Default to full image if bbox missing\n    bbox = np.clip(np.array(bbox) / 224.0, 0.0, 1.0)  # Normalize and clip to [0, 1]\n\n    # Handle missing or invalid labels\n    label[\"instrument_id\"] = 6 if label.get(\"instrument_id\", -1) == -1 else max(label.get(\"instrument_id\", -1), 0)\n    label[\"verb_id\"] = max(label.get(\"verb_id\", -1), 0)\n    label[\"target_id\"] = max(label.get(\"target_id\", -1), 0)\n    label[\"triplet_id\"] = 100 if label.get(\"triplet_id\", -1) == -1 else max(label.get(\"instrument_id\", -1), 0)\n\n    return image, bbox, label\n\ndef create_tf_dataset(dataframe, batch_size, num_instruments, num_verbs, num_targets, num_triplets):\n    def generator():\n        for _, row in dataframe.iterrows():\n            try:\n                bbox = row.get(\"bbox\", [0.0, 0.0, 1.0, 1.0])\n                if len(bbox) != 4:\n                    bbox = [0.0, 0.0, 1.0, 1.0]  # Default bounding box\n    \n                image, bbox, labels = preprocess_image(\n                    row[\"frame_path\"], \n                    {\n                        \"instrument_id\": int(row[\"instrument_id\"]),\n                        \"verb_id\": int(row[\"verb_id\"]),\n                        \"target_id\": int(row[\"target_id\"]),\n                        \"triplet_id\": int(row[\"triplet_id\"]),\n                        \"bbox\": bbox\n                    }, \n                    num_instruments, num_verbs, num_targets, num_triplets\n                )\n    \n                bbox = np.array(bbox, dtype=np.float32)\n                if bbox.shape != (4,):\n                    print(f\"Invalid bbox shape: {bbox.shape}, using default [0.0, 0.0, 1.0, 1.0]\")\n                    bbox = np.array([0.0, 0.0, 1.0, 1.0], dtype=np.float32)\n    \n                yield (image, bbox), (\n                    labels[\"instrument_id\"],\n                    labels[\"verb_id\"],\n                    labels[\"target_id\"],\n                    labels[\"triplet_id\"]\n                )\n            except Exception as e:\n                print(f\"Error processing row: {e}\")\n                continue\n\n    dataset = tf.data.Dataset.from_generator(\n        generator,\n        output_signature=(\n            (\n                tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n                tf.TensorSpec(shape=(4,), dtype=tf.float32),\n            ),\n            (\n                tf.TensorSpec(shape=(), dtype=tf.int32),\n                tf.TensorSpec(shape=(), dtype=tf.int32),\n                tf.TensorSpec(shape=(), dtype=tf.int32),\n                tf.TensorSpec(shape=(), dtype=tf.int32),\n            )\n        )\n    )\n    return dataset.repeat().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n# Custom Layer for Scaling Bounding Box\nclass ScaleBBox(Layer):\n    def __init__(self, target_size):\n        super(ScaleBBox, self).__init__()\n        self.target_size = target_size\n\n    def call(self, bbox):\n        x_min, y_min, x_max, y_max = tf.unstack(bbox, axis=-1)\n        width = self.target_size[1]\n        height = self.target_size[0]\n        \n        scaled_bbox = tf.stack([\n            x_min * width,\n            y_min * height,\n            x_max * width,\n            y_max * height\n        ], axis=-1)\n        \n        return scaled_bbox\n\nclass CropToBBox(Layer):\n    def __init__(self):\n        super(CropToBBox, self).__init__()\n\n    def call(self, inputs):\n        image, bbox = inputs\n\n        # Debug statement for checking the input sizes\n        print(f\"Image shape before crop: {tf.shape(image)}\")\n        print(f\"Bounding box shape: {tf.shape(bbox)}\")\n\n        # Cast height and width to float32 for the multiplication with float values in bbox\n        height = tf.cast(tf.shape(image)[1], tf.float32)\n        width = tf.cast(tf.shape(image)[2], tf.float32)\n\n        y_min, x_min, y_max, x_max = tf.split(bbox, num_or_size_splits=4, axis=-1)\n        \n        # Normalize bbox values\n        y_min = y_min / height\n        x_min = x_min / width\n        y_max = y_max / height\n        x_max = x_max / width\n\n        normalized_bbox = tf.concat([y_min, x_min, y_max, x_max], axis=-1)\n\n        # Use tf.map_fn to handle batch processing\n        cropped_images = tf.map_fn(\n            lambda i: self._crop_and_resize(image[i], normalized_bbox[i], height, width),\n            elems=tf.range(tf.shape(image)[0]), dtype=tf.float32\n        )\n\n        # Debug statement for output shape\n        print(f\"Cropped image shape: {tf.shape(cropped_images)}\")\n\n        return cropped_images\n\n    def _crop_and_resize(self, image, bbox, height, width):\n        y_min, x_min, y_max, x_max = tf.unstack(bbox)\n        \n        # Cast height and width to float32 for calculation purposes\n        height = tf.cast(height, tf.float32)\n        width = tf.cast(width, tf.float32)\n    \n        # Use tf.cast for indexing (conversion from float to int)\n        y_min = tf.cast(y_min * height, tf.int32)\n        x_min = tf.cast(x_min * width, tf.int32)\n        y_max = tf.cast(y_max * height, tf.int32)\n        x_max = tf.cast(x_max * width, tf.int32)\n        \n        # Get the region to crop based on bbox\n        crop = image[y_min:y_max, x_min:x_max]\n        \n        # Resize the crop to the target size\n        resized_crop = tf.image.resize(crop, (224, 224))\n    \n        return resized_crop\n\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0][0], 224, 224, input_shape[0][-1])\n\n\n# Define the model\ndef build_model(num_instruments, num_verbs, num_targets, num_triplets):\n    image_input = Input(shape=(None, None, 3), name=\"image_input\")\n    resize_layer = Resizing(224, 224)(image_input)\n    bbox_input = Input(shape=(4,), name=\"bbox_input\")\n\n    scale_bbox_layer = ScaleBBox(target_size=(224, 224))\n    bbox_input_scaled = scale_bbox_layer(bbox_input)\n\n    crop_to_bbox_layer = CropToBBox()\n    cropped_image = crop_to_bbox_layer([image_input, bbox_input_scaled])\n\n    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n    x = base_model(cropped_image)\n    x = GlobalAveragePooling2D()(x)\n\n    x = Concatenate()([x, bbox_input_scaled])\n\n    instrument_out = Dense(num_instruments, activation=\"softmax\", name=\"instrument_id\")(x)\n    verb_out = Dense(num_verbs, activation=\"softmax\", name=\"verb_id\")(x)\n    target_out = Dense(num_targets, activation=\"softmax\", name=\"target_id\")(x)\n    triplet_out = Dense(num_triplets, activation=\"softmax\", name=\"triplet_id\")(x)\n\n    return tf.keras.Model(inputs=[image_input, bbox_input], outputs=[instrument_out, verb_out, target_out, triplet_out])\n\n# Compile the model\nmodel = build_model(len(instrument_classes), len(verb_classes), len(target_classes), len(triplet_classes))\n\n# Add gradient clipping\noptimizer = Adam(learning_rate=1e-4, clipvalue=1.0)\n\nmodel.compile(\n    optimizer=optimizer,\n    loss={\n        \"instrument_id\": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        \"verb_id\": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        \"target_id\": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        \"triplet_id\": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n    },\n    metrics={\n        \"instrument_id\": \"accuracy\",\n        \"verb_id\": \"accuracy\",\n        \"target_id\": \"accuracy\",\n        \"triplet_id\": \"accuracy\",\n    },\n)\n\n# Update the dataset creation with the modified class counts\ntrain_dataset = create_tf_dataset(train_df, batch_size=16, num_instruments=len(instrument_classes), num_verbs=len(verb_classes), num_targets=len(target_classes), num_triplets=len(triplet_classes))\nval_dataset = create_tf_dataset(test_df, batch_size=16, num_instruments=len(instrument_classes), num_verbs=len(verb_classes), num_targets=len(target_classes), num_triplets=len(triplet_classes))\n\n# Force the entire model to use CPU\n# with tf.device('/CPU:0'):\nmodel.fit(train_dataset, validation_data=val_dataset, epochs=10, steps_per_epoch=10, validation_steps=50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:06:19.297321Z","iopub.execute_input":"2025-01-14T10:06:19.297802Z","iopub.status.idle":"2025-01-14T10:09:18.427492Z","shell.execute_reply.started":"2025-01-14T10:06:19.297758Z","shell.execute_reply":"2025-01-14T10:09:18.426643Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\nImage shape before crop: Tensor(\"functional_13_1/crop_to_b_box_13_1/Shape:0\", shape=(4,), dtype=int32)\nBounding box shape: Tensor(\"functional_13_1/crop_to_b_box_13_1/Shape_1:0\", shape=(2,), dtype=int32)\nCropped image shape: Tensor(\"functional_13_1/crop_to_b_box_13_1/Shape_5:0\", shape=(4,), dtype=int32)\nImage shape before crop: Tensor(\"functional_13_1/crop_to_b_box_13_1/Shape:0\", shape=(4,), dtype=int32)\nBounding box shape: Tensor(\"functional_13_1/crop_to_b_box_13_1/Shape_1:0\", shape=(2,), dtype=int32)\nCropped image shape: Tensor(\"functional_13_1/crop_to_b_box_13_1/Shape_5:0\", shape=(4,), dtype=int32)\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - instrument_id_accuracy: 0.3123 - loss: 10.7734 - target_id_accuracy: 0.1915 - triplet_id_accuracy: 0.1147 - verb_id_accuracy: 0.1389  Image shape before crop: Tensor(\"functional_13_1/crop_to_b_box_13_1/Shape:0\", shape=(4,), dtype=int32)\nBounding box shape: Tensor(\"functional_13_1/crop_to_b_box_13_1/Shape_1:0\", shape=(2,), dtype=int32)\nCropped image shape: Tensor(\"functional_13_1/crop_to_b_box_13_1/Shape_5:0\", shape=(4,), dtype=int32)\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 2s/step - instrument_id_accuracy: 0.3260 - loss: 10.5391 - target_id_accuracy: 0.2081 - triplet_id_accuracy: 0.1321 - verb_id_accuracy: 0.1570 - val_instrument_id_accuracy: 0.0000e+00 - val_loss: 12.6794 - val_target_id_accuracy: 0.0000e+00 - val_triplet_id_accuracy: 0.0000e+00 - val_verb_id_accuracy: 0.0000e+00\nEpoch 2/10\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - instrument_id_accuracy: 0.5881 - loss: 3.0850 - target_id_accuracy: 0.5881 - triplet_id_accuracy: 0.5881 - verb_id_accuracy: 0.5881 - val_instrument_id_accuracy: 0.0000e+00 - val_loss: 11.5216 - val_target_id_accuracy: 1.0000 - val_triplet_id_accuracy: 0.0000e+00 - val_verb_id_accuracy: 0.0000e+00\nEpoch 3/10\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - instrument_id_accuracy: 0.5405 - loss: 2.7253 - target_id_accuracy: 0.5405 - triplet_id_accuracy: 0.5405 - verb_id_accuracy: 0.5405 - val_instrument_id_accuracy: 0.0000e+00 - val_loss: 11.4201 - val_target_id_accuracy: 1.0000 - val_triplet_id_accuracy: 0.0000e+00 - val_verb_id_accuracy: 0.0000e+00\nEpoch 4/10\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - instrument_id_accuracy: 0.6244 - loss: 2.2073 - target_id_accuracy: 0.6244 - triplet_id_accuracy: 0.6244 - verb_id_accuracy: 0.6244 - val_instrument_id_accuracy: 0.0000e+00 - val_loss: 11.6776 - val_target_id_accuracy: 1.0000 - val_triplet_id_accuracy: 0.0000e+00 - val_verb_id_accuracy: 0.0000e+00\nEpoch 5/10\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - instrument_id_accuracy: 0.5566 - loss: 2.5456 - target_id_accuracy: 0.5566 - triplet_id_accuracy: 0.5566 - verb_id_accuracy: 0.5566 - val_instrument_id_accuracy: 0.0000e+00 - val_loss: 11.1941 - val_target_id_accuracy: 1.0000 - val_triplet_id_accuracy: 0.0000e+00 - val_verb_id_accuracy: 0.0000e+00\nEpoch 6/10\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - instrument_id_accuracy: 0.6904 - loss: 1.8137 - target_id_accuracy: 0.6904 - triplet_id_accuracy: 0.6904 - verb_id_accuracy: 0.6904 - val_instrument_id_accuracy: 0.0000e+00 - val_loss: 11.0136 - val_target_id_accuracy: 1.0000 - val_triplet_id_accuracy: 0.0000e+00 - val_verb_id_accuracy: 0.0000e+00\nEpoch 7/10\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - instrument_id_accuracy: 0.5881 - loss: 2.3835 - target_id_accuracy: 0.5881 - triplet_id_accuracy: 0.5881 - verb_id_accuracy: 0.5881 - val_instrument_id_accuracy: 0.0000e+00 - val_loss: 10.3886 - val_target_id_accuracy: 1.0000 - val_triplet_id_accuracy: 0.0000e+00 - val_verb_id_accuracy: 0.0000e+00\nEpoch 8/10\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - instrument_id_accuracy: 0.5405 - loss: 2.7307 - target_id_accuracy: 0.5405 - triplet_id_accuracy: 0.5405 - verb_id_accuracy: 0.5405 - val_instrument_id_accuracy: 0.0000e+00 - val_loss: 10.1756 - val_target_id_accuracy: 1.0000 - val_triplet_id_accuracy: 0.0000e+00 - val_verb_id_accuracy: 0.0000e+00\nEpoch 9/10\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - instrument_id_accuracy: 0.6244 - loss: 2.2563 - target_id_accuracy: 0.6244 - triplet_id_accuracy: 0.6244 - verb_id_accuracy: 0.6244 - val_instrument_id_accuracy: 0.0000e+00 - val_loss: 10.8703 - val_target_id_accuracy: 1.0000 - val_triplet_id_accuracy: 0.0000e+00 - val_verb_id_accuracy: 0.0000e+00\nEpoch 10/10\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - instrument_id_accuracy: 0.5566 - loss: 2.5667 - target_id_accuracy: 0.5566 - triplet_id_accuracy: 0.5566 - verb_id_accuracy: 0.5566 - val_instrument_id_accuracy: 0.0000e+00 - val_loss: 10.6414 - val_target_id_accuracy: 1.0000 - val_triplet_id_accuracy: 0.0000e+00 - val_verb_id_accuracy: 0.0000e+00\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7f5620143190>"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"# import tensorflow as tf\n# import numpy as np\n# import os\n# from tensorflow.keras import Model\n# from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Concatenate\n# from tensorflow.keras.applications import ResNet50\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.callbacks import Callback\n# from PIL import Image\n\n# # Data loading and preprocessing with bbox handling\n\n\n# # Define model with multi-task learning\n# def build_model(num_instruments, num_verbs, num_targets, num_triplets):\n#     base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n#     x = GlobalAveragePooling2D()(base_model.output)\n\n#     # Concatenate bounding box coordinates\n#     bbox_input = Input(shape=(4,), name=\"bbox_input\")  # Bounding box coordinates\n#     x = Concatenate()([x, bbox_input])  # Concatenate bbox coordinates with image features\n\n#     # Classification branches\n#     instrument_out = Dense(num_instruments, activation=\"softmax\", name=\"instrument_id\")(x)\n#     verb_out = Dense(num_verbs, activation=\"softmax\", name=\"verb_id\")(x)\n#     target_out = Dense(num_targets, activation=\"softmax\", name=\"target_id\")(x)\n#     triplet_out = Dense(num_triplets, activation=\"softmax\", name=\"triplet_id\")(x)\n\n#     return Model(inputs=[base_model.input, bbox_input], outputs=[instrument_out, verb_out, target_out, triplet_out])\n\n# # Compile the model\n# model = build_model(len(instrument_classes), len(verb_classes), len(target_classes), len(triplet_classes))\n\n# # Add gradient clipping to prevent NaNs\n# optimizer = Adam(learning_rate=1e-4)\n# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, clipvalue=1.0)  # Clip gradients to avoid exploding gradients\n\n# model.compile(\n#     optimizer=optimizer,\n#     loss={\n#         \"instrument_id\": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n#         \"verb_id\": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n#         \"target_id\": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n#         \"triplet_id\": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n#     },\n#     metrics={\n#         \"instrument_id\": \"accuracy\",\n#         \"verb_id\": \"accuracy\",\n#         \"target_id\": \"accuracy\",\n#         \"triplet_id\": \"accuracy\",\n#     },\n# )\n# # Modify the training loop or validation to print labels and model outputs\n# class NaNCheckCallback(Callback):\n#     def on_batch_end(self, batch, logs=None):\n#         if np.any(np.isnan(logs['loss'])):\n#             print(\"NaN loss detected at batch:\", batch)\n#             self.model.stop_training = True\n#         if batch == 0:  # Only check once at the beginning of training\n#             print(\"Batch 0 outputs and labels:\")\n#             outputs = self.model.predict(self.model.inputs)\n#             print(\"Outputs:\", outputs)\n#             print(\"Labels:\", self.model.targets)\n\n# # model.fit(train_dataset, validation_data=val_dataset, epochs=10, steps_per_epoch=7, callbacks=[NaNCheckCallback()])\n\n# # Create datasets\n# train_dataset = create_tf_dataset(train_df, batch_size=16, num_instruments=len(instrument_classes), \n#                                    num_verbs=len(verb_classes), num_targets=len(target_classes), \n#                                    num_triplets=len(triplet_classes))\n\n# val_dataset = create_tf_dataset(test_df, batch_size=16, num_instruments=len(instrument_classes), \n#                                  num_verbs=len(verb_classes), num_targets=len(target_classes), \n#                                  num_triplets=len(triplet_classes))\n# # print(train_dataset.type)\n# # Add repeat function to avoid running out of data\n# train_dataset = train_dataset.repeat()\n\n# # Train the model with NaNCheckCallback\n# model.fit(train_dataset, validation_data=val_dataset, epochs=10, steps_per_epoch=7, callbacks=[NaNCheckCallback()])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T06:06:34.591853Z","iopub.execute_input":"2025-01-14T06:06:34.592172Z","iopub.status.idle":"2025-01-14T06:06:34.596115Z","shell.execute_reply.started":"2025-01-14T06:06:34.592145Z","shell.execute_reply":"2025-01-14T06:06:34.595323Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport json\nfrom PIL import Image\nimport os\n\n# Assuming the trained model is loaded\n# model = tf.keras.models.load_model('path_to_trained_model')\n\ndef make_predictions(test_df):\n    video_predictions = {}\n\n    for video_id in test_df['video_id'].unique():  # Loop through each video in the test data\n        video_frames = test_df[test_df['video_id'] == video_id]\n        frame_predictions = {}\n\n        for _, frame_data in video_frames.iterrows():  # Iterate over each frame of the video\n            frame_id = frame_data['frame_id']\n            frame_path = frame_data['frame_path']\n            bbox = frame_data['bbox']  # Assuming bounding boxes are in 'bbox' column\n            \n            # Load image from frame_path\n            image = load_image(frame_path)\n            \n            # Preprocess the image\n            processed_image = preprocess_image(image)\n            \n            # Make predictions for recognition (probabilities of the triplets)\n            recognition_probabilities = model.predict(processed_image)  # Output is a probability vector\n            \n            # Get detection predictions (triplets, instrument ids, probabilities, and bounding boxes)\n            detection_predictions = detect_instruments(processed_image, bbox)  # Assuming a detection function is defined\n\n            # Format frame predictions\n            frame_predictions[frame_id] = {\n                \"recognition\": recognition_probabilities.tolist(),\n                \"detection\": detection_predictions\n            }\n\n        # Store predictions for the video\n        video_predictions[video_id] = frame_predictions\n    \n    return video_predictions\n\ndef load_image(frame_path):\n    \"\"\"Load image from the given path.\"\"\"\n    try:\n        image = Image.open(frame_path)\n        image = image.convert(\"RGB\")  # Ensure image is in RGB format\n        return np.array(image)\n    except Exception as e:\n        print(f\"Error loading image {frame_path}: {e}\")\n        return None\n\ndef preprocess_image(image):\n    \"\"\"Preprocess the image (resize, normalize, etc.).\"\"\"\n    if image is not None:\n        # Resize image to the required input shape\n        image = tf.image.resize(image, (224, 224))  \n        image = image / 255.0  # Normalize if needed\n        return np.expand_dims(image, axis=0)  # Add batch dimension\n    return np.zeros((1, 224, 224, 3))  # Return a dummy image if loading failed\n\ndef detect_instruments(image, bbox):\n    \"\"\"Detect instruments based on bounding box and image.\"\"\"\n    detections = []\n    if bbox:  # If bounding box exists, use it for detection\n        for triplet in range(100):  # Assuming you have 100 triplets\n            tool_id = triplet  # Replace with actual logic for tool_id prediction\n            tool_prob = np.random.rand()  # Random probability for now, replace with actual prediction\n            detection = {\n                \"triplet\": triplet,\n                \"instrument\": [tool_id, tool_prob, *bbox]\n            }\n            detections.append(detection)\n    return detections\n\n# Example: Generate the prediction results\ntest_predictions = make_predictions(test_df)\n\n# Save the results in the required JSON format\noutput_filename = \"MODEL_NAME.json\"  # Replace with your model name\nwith open(output_filename, 'w') as f:\n    json.dump(test_predictions, f, indent=4)\n\nprint(f\"Predictions saved to {output_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:15:03.991575Z","iopub.execute_input":"2025-01-14T10:15:03.992024Z","iopub.status.idle":"2025-01-14T10:15:04.084372Z","shell.execute_reply.started":"2025-01-14T10:15:03.991986Z","shell.execute_reply":"2025-01-14T10:15:04.083010Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'frame_id'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-e6c8bf028d35>\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m# Example: Generate the prediction results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Save the results in the required JSON format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-50-e6c8bf028d35>\u001b[0m in \u001b[0;36mmake_predictions\u001b[0;34m(test_df)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvideo_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Iterate over each frame of the video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mframe_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frame_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mframe_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frame_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bbox'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Assuming bounding boxes are in 'bbox' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0;31m# Convert generator to list before going through hashable part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3796\u001b[0m             ):\n\u001b[1;32m   3797\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3798\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3799\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'frame_id'"],"ename":"KeyError","evalue":"'frame_id'","output_type":"error"}],"execution_count":50},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T19:41:25.631289Z","iopub.execute_input":"2025-01-13T19:41:25.631605Z","iopub.status.idle":"2025-01-13T19:41:25.893996Z","shell.execute_reply.started":"2025-01-13T19:41:25.631584Z","shell.execute_reply":"2025-01-13T19:41:25.893092Z"}},"outputs":[{"name":"stdout","text":"Mon Jan 13 19:41:25 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   34C    P0             32W /  250W |    3151MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"tf.config.list_physical_devices('GPU')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T19:40:03.562236Z","iopub.execute_input":"2025-01-13T19:40:03.562585Z","iopub.status.idle":"2025-01-13T19:40:03.568848Z","shell.execute_reply.started":"2025-01-13T19:40:03.562561Z","shell.execute_reply":"2025-01-13T19:40:03.567709Z"}},"outputs":[{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"},"metadata":{}}],"execution_count":102},{"cell_type":"code","source":"print(train_df.columns)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:19:05.774705Z","iopub.execute_input":"2025-01-13T09:19:05.775009Z","iopub.status.idle":"2025-01-13T09:19:05.779359Z","shell.execute_reply.started":"2025-01-13T09:19:05.774983Z","shell.execute_reply":"2025-01-13T09:19:05.778432Z"}},"outputs":[{"name":"stdout","text":"Index(['video_id', 'frame_path', 'triplet_id', 'instrument_id', 'verb_id',\n       'target_id', 'phase_id', 'bbox'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":100},{"cell_type":"code","source":"import pytorch_lightning as pl\nfrom torch.utils.data import DataLoader\n\nclass SurgicalDataModule(pl.LightningDataModule):\n    def __init__(self, train_dataset, test_dataset, batch_size=32):\n        super().__init__()\n        self.train_dataset = train_dataset\n        self.test_dataset = test_dataset\n        self.batch_size = batch_size\n\n    def train_dataloader(self):\n        # Create DataLoader for training dataset\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n\n    def val_dataloader(self):\n        # Create DataLoader for validation/test dataset\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n\n# # Example of how you can use it\n# # Make sure you define `train_dataset` and `test_dataset` before using the data module\n# train_dataset = ...  # Define your training dataset\n# test_dataset = ...   # Define your testing dataset\n\ndata_module = SurgicalDataModule(train_dataset, test_dataset, batch_size=16)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:17:44.420235Z","iopub.execute_input":"2025-01-13T13:17:44.420549Z","iopub.status.idle":"2025-01-13T13:17:44.426117Z","shell.execute_reply.started":"2025-01-13T13:17:44.420525Z","shell.execute_reply":"2025-01-13T13:17:44.425137Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"import os\nimport torch\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport pytorch_lightning as pl\n\n# Disable CUDA completely\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n# Define the checkpoint callback\ncheckpoint_callback = ModelCheckpoint(\n    monitor=\"val_loss\",  # Monitor validation loss\n    dirpath=\"./checkpoints\",  # Save to this directory\n    filename=\"triplet_model-{epoch:02d}-{val_loss:.2f}\",\n    save_top_k=1,  # Save only the best model\n    mode=\"min\",  # Minimize validation loss\n)\n\n# Define the Lightning model\nclass MultiTaskTripletLightningModel(pl.LightningModule):\n    def __init__(self, num_instruments, num_verbs, num_targets, num_triplets):\n        super().__init__()\n        # Ensure the model is initialized for CPU\n        self.model = ...  # Define your model layers\n\n    def forward(self, x):\n        x = x.to(\"cpu\")  # Ensure input tensor is on the CPU\n        return self.model(x)\n\n# Instantiate the Lightning model\nlightning_model = MultiTaskTripletLightningModel(\n    num_instruments=len(instrument_classes),\n    num_verbs=len(verb_classes),\n    num_targets=len(target_classes),\n    num_triplets=len(triplet_classes)\n)\n\n# Move model to CPU explicitly\nlightning_model.to(\"cpu\")\n\ntrainer = pl.Trainer(\n    max_epochs=20,\n    accelerator=\"cpu\",  # Ensure CPU usage\n    devices=1,           # Use only one CPU\n    log_every_n_steps=20,\n    enable_checkpointing=False,  # Disable checkpointing\n    enable_progress_bar=True,  # Enable progress bar\n)\n\n# Fit the model with the data module\ntrainer.fit(lightning_model, datamodule=data_module)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:17:47.481059Z","iopub.execute_input":"2025-01-13T13:17:47.481341Z","iopub.status.idle":"2025-01-13T13:17:47.545279Z","shell.execute_reply.started":"2025-01-13T13:17:47.481321Z","shell.execute_reply":"2025-01-13T13:17:47.544211Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-744c6cee5424>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Fit the model with the data module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlightning_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         )\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attach_model_logging_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m         \u001b[0m_verify_loop_configurations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py\u001b[0m in \u001b[0;36m_verify_loop_configurations\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected: Trainer state fn must be set before validating loop configuration.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrainerFn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFITTING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0m__verify_train_val_loop_configuration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0m__verify_manual_optimization_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrainerFn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVALIDATING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py\u001b[0m in \u001b[0;36m__verify_train_val_loop_configuration\u001b[0;34m(trainer, model)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mhas_training_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_overridden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_training_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         raise MisconfigurationException(\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;34m\"No `training_step()` method defined. Lightning `Trainer` expects as minimum a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;34m\" `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMisconfigurationException\u001b[0m: No `training_step()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined."],"ename":"MisconfigurationException","evalue":"No `training_step()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.","output_type":"error"}],"execution_count":46},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}